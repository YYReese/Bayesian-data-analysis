---
editor_options:
  markdown:
    wrap: 72
output: pdf_document
---

**University of Edinburgh**

**School of Mathematics**

**Bayesian Data Analysis, 2022/2023, Semester 2**

**Assignment 1**

```{r}
rm(list = ls(all = TRUE))
#Do not delete this!
#It clears all variables to ensure reproducibility
library(dbplyr)
library(tidyverse)
library(rjags)
library(INLA)
```

![](Exchange-rate.jpg)

**Problem 1**

**In this problem, we study a dataset about currency exchange rates. The
exrates dataset of the stochvol package contains the daily average
exchange rates of 24 currencies versus the EUR, from 2000-01-03 until
2012-04-04.**

```{r}
set.seed(1234)
require(stochvol)
data("exrates")

#The first 6 rows of the dataframe
print.data.frame(exrates[1:6,])

cat(paste("Data from ", min(exrates$date)," until ",max(exrates$date)))
```

**As we can see, not all dates are included in the dataset. Some are
missing, such as weekends, and public holidays.**

**In this problem, we are going to fit a various stochastic volatility
models on this dataset (see e.g.
<https://www.jstor.org/stable/1392251>).**

**a)[10 marks] Consider the following leveraged Stochastic Volatility
(SV) model.**

$\begin{aligned} y_t&=\beta_0+\beta_1 y_{t-1}+\exp(h_t/2)\epsilon_t \quad \text{for}\quad 1\le t\le T,\\ h_{t+1}&=\mu+\phi(h_t-\mu)+\sigma \eta_t\quad \text{for} \quad 0\le t\le T, \quad h_0\sim N(\mu, \sigma^2/(1-\phi^2)),\\(\epsilon_t,\eta_t)&\sim N\left(0, \Sigma_{\rho}\right)\quad \text{ for } \quad \Sigma_{\rho}=\left(\begin{matrix}1 & \rho\\ \rho & 1\end{matrix}\right). \end{aligned}$

**Here** $t$ **is the time index,** $y_t$ **are the observations (such
as daily USD/EUR rate),** $h_t$ **are the log-variance process,**
$\epsilon_t$ **is the observation noise, and** $\eta_t$ **is the
log-variance process noise (which are correlated, but independent for
different values of** \$t\$**). The hyperparameters are**
$\beta_0, \beta_1, \mu, \phi, \sigma, \rho$**.**

**For stability, it is necessary to have** $\phi\in (-1,1)$**, and by
the definition of correlation matrices, we have** $\rho\in [-1,1]$**.**

**Implement this model in JAGS or Stan on the first 3 months of USD/EUR
data from the dataset, i.e. from dates 2000-01-03 until 2000-04-02.**

**Explain how did you choose priors for all parameters. Explain how did
you take into account the days without observation in your model.**

**Fit the model, do convergence diagnostics, print out the summary of
the results, and discuss them.**

**Make sure that the Effective Sample Size is at least 1000 for all 6
hyperparameters (you need to choose burn-in and number of steps
appropriately for this).**

## Data exploration

First, we extract the first 3 months of USD/EUR data from the dataset,
and insert new entries to where the data is missing with NA in order to
include all dates from 2000-01-03 until 2000-04-02. Then we create a
response variable y by storing the logarithm of the USD/EUR exchange
rate estimate for each day, with NA if they are missing. Below is the
plot of the response variable y we just created. We see there is a
decreasing trend with some fluctuations.

```{r}
# Create data input for JAGS
eur2usd <- exrates %>% 
  filter(date <= "2000/04/02") %>%
  mutate(date=as.numeric(date)) %>%
  select(USD,date) %>%
  mutate(date=date-date[1]+1,USD=log(USD))

n <- eur2usd$date[length(eur2usd$date)]
y <- rep(NA,n)
j <- 1
for(i in eur2usd$date){
  y[i] <- eur2usd$USD[j]
  j <- j+1
}
#y <- y-mean(y,na.rm = TRUE)
plot(y)
```

## Leverage Stochastic Volatility model

Let $\boldsymbol{\theta}$ denote the parameter set,
$\boldsymbol{\theta}=\{\beta_0, \beta_1, \phi,\rho,\sigma\}$. Since
$\epsilon_t$ and $\eta_t$ given $\boldsymbol{\theta}$ are jointly
Gaussian, $y_t$ and $h_{t+1}$ given $h_t$ and $\boldsymbol{\theta}$ are
also jointly Gaussian with mean and covariance matrix given below due to
the linearity. $$
\left(\begin{matrix} y_t \\ h_{t+1} \end{matrix}\right)\bigg|_{h_t,y_{t-1},\boldsymbol{\theta}} \sim \mathcal{N} \left( \left[ \begin{matrix} \beta_0+\beta_1y_{t-1} \\ mu+\phi(h_t-\mu) \end{matrix} \right] , \left[ \begin{matrix} \exp(h_t) & \sigma \rho \exp(\frac{h_t}{2})\\\sigma \rho \exp(\frac{h_t}{2})& \sigma^2 \end{matrix} \right] \right)
$$ By conditioning, the leverage stochastic volatility model can be
rewritten as \$\$ h\_{t+1} \| \boldsymbol{\theta}
\sim \mathcal{N}(\mu+\phi(h_t-\mu),\sigma\^2) \\

y_t\|h\_{t+1},h\_{t},y\_{t-1},\boldsymbol{\theta} \sim \mathcal{N}
\left(\beta*0+*\beta*1y*{t-1}+\exp(\frac{h_t}{2})(h{t+1}-\mu-\phi(h_t-\mu)),(1-\rho\^2)\exp(h_t)\right)

\$\$ ##\# Prior specification $\beta_0,\beta_1$: For coefficients
$\beta_0$ and $\beta_1$, it is natural to set the Normal prior on it. As
we do not have much prior belief, large variances are suggested.
$\beta_0 \sim \mathcal{N}(0,1000)$. $\beta_1 \sim \mathcal{N}(0,1000)$

$\mu$: A vague with mean $0$ and large variance (e.g.$1000$) is used for
$\mu$, $\mu \sim \mathcal {N} (0,1000)$.

$\phi$: As it is necessary to have $\phi \in (-1,1)$ for stability, a
prior that ranges from $-1$ to $1$ is needed. A Beta prior has been
widely used for parameters ranging from $0$ to $1$. If we put a Beta
prior on $\frac{\phi+1}{2}$, we then have $0<\frac{\phi+1}{2}<1$.
Consequently, $-1<\phi<1$ is ensured. Therefore, the prior for $\phi$ is
chosen to be $\frac{\phi+1}{2} \sim \text{Beta} (1,1)$.

$\sigma$: $\sigma \sim \text{Gamma} (0, 1000)$

$\rho$: An uninformative prior is used for $\rho$. In particular sine
$\rho\in [-1,1]$, we use $\rho \sim \text{Uniform} (-1,1)$.

We have implemented the model in JAGS as shown below. The hidden states
are stored in the vector $h$, and the observations are stored in the
vector $y$. We pass along $y$ created above to JAGS as $y$ when
compiling the model. In this $y$ variable (which is a vector of length
$n$), remember that we have already set all the points with missing data
as NA. JAGS is able to handle these NA values automatically by creating
stochastic nodes for them (and we could even obtain MCMC samples from
them). We ran $5$ chains with $50000$ samples after $20000$ burn-in
iterations, starting from an initial position for $y$ based on the true
observation of $y_1$ and some random noise.

```{r}
#model in BUGS syntax

model_string <- "model {
  # prior
  beta0 ~ dnorm(beta.mu.0, beta.tau.0)
  beta1 ~ dnorm(beta.mu.1, beta.tau.1)
  mu ~ dnorm(mu.mu,mu.tau)
  trans_phi ~ dbeta(20,1.5)
  phi <- 2*trans_phi-1
  tau ~ dgamma(sigma.a,sigma.b) 
  sigma <- pow(tau,-1/2)
  trans_rho ~ dbeta(3,5)
  rho <- 2*trans_phi-1
  # Likelihood
  h[1] ~ dnorm(mu, (1-phi^2)*tau)
  #The evolution of the hidden states x according to the model
  for(i in 2:(n+1)) {
    h[i] ~ dnorm(mu+phi*(h[i-1]-mu), tau) 
  }
  
  y[1] ~ dnorm(mean.y0,prec.y0)
  y.var[1] <- prec.y0^(-1)
  for(i in 2:n) {
    y.mu[i] <- rho/sigma*exp(h[i]/2)*(h[i+1]-mu-phi*(h[i]-mu))+beta0 + beta1*y[i-1]
    y.var[i] <- exp(h[i])*(1-rho^2) 
    y[i] ~ dnorm(y.mu[i], pow(y.var[i],-1))
  }
  


}"
```

```{r}
# Create data input for JAGS
eur2usd <- exrates %>% 
  filter(date <= "2000/04/02") %>%
  mutate(date=as.numeric(date)) %>%
  select(USD,date) %>%
  mutate(date=date-date[1]+1,USD=log(USD))

n <- eur2usd$date[length(eur2usd$date)] +2 # include 1/4 and 2/4
y <- rep(NA,n) 
j <- 1
for(i in eur2usd$date){
  y[i] <- eur2usd$USD[j]
  j <- j+1
}


eur2usd.data <- list(n=n,y=y,mean.y0=y[1],prec.y0=0.001,
                     beta.mu.0=0,beta.tau.0=0.001,
                     beta.mu.1=0,beta.tau.1=0.001,
                     mu.mu=0,mu.tau=0.001,sigma.a=1,sigma.b=1)


model1.inits=list()
beta0.init=10
beta1.init=0.5
mu.init=0
phi.init=0.1
tau.init=100
rho.init=0

for(it in 1:5)
{
  model1.inits[[it]]<-list(beta0=beta0.init+rnorm(1,sd=1),beta1=beta1.init+rnorm(1,sd=1),
                           mu=mu.init+rnorm(1,sd=1),phi=phi.init+rnorm(1,sd=1),
                           tau=tau.init+rnorm(1,1), rho=rho.init+rnorm(1,1))
}

```

```{r}
num.chains <- 5
# Compile and run the model
model1 <- jags.model(file=textConnection(model_string), 
                        data=eur2usd.data,
                        n.chains=num.chains)
# Burnin for 50000 samples
update(model1,50000)
```

Explanation: We printed out the posterior summaries for the model
parameters beta0, beta1, mu, phi, rho, and sigma using the
`summary(res.model1)` command.
\textcolor{red}{We can see that the standard deviations are typically much
smaller than the means in absolute value, indicating that the the amount
of data is sufficient to fit the model parameters with some degree of
confidence.} The time series SE is smaller than the posterior means (in
absolute value), showing that our estimates are relatively accurate. The
trace plots of each parameter show that the chains were mixing well.

```{r}
# Running the model, monitoring the variable theta
res.model1=coda.samples(model1,variable.names=c("beta0", "beta1", "mu", "phi", "sigma", "rho"), n.iter=60000,progress.bar="none")

#Setting margins to be small
summary(res.model1); effectiveSize(res.model1);
plot(res.model1)
```

Explanation: We plotted the autocorrelation functions for the 4
components of A using the acf function. We set the lag.max parameter to
1000 to clearly display the plots. We can see that the samples from mu
suffer from the most autocorrelation, but it is mostly gone after a lag
of 100, and almost completely gone after a lag of 300.

```{r}
par(mar = c(2, 2, 4, 2))
acf(res.model1[[1]][,"beta0"],lag.max=1000,main="ACF for beta0")
acf(res.model1[[1]][,"beta1"],lag.max=1000,main="ACF for beta1")
acf(res.model1[[1]][,"mu"],lag.max=5000,main="ACF for mu")
acf(res.model1[[1]][,"phi"],lag.max=1000,main="ACF for phi")
acf(res.model1[[1]][,"rho"],lag.max=1000,main="ACF for rho")
```

```{r}
dic.samples(model1,n.iter=10000)
```

```{r}

gelman.diag(res.model1)

```

```{r}
gelman.plot(res.model1)
```

**b)[10 marks] In practice, one often encounters outliers in exchange
rates. These can be sometimes modeled by assuming Student's t
distribution in the observation errors (i.e.** $\epsilon_t$). **The
robust leveraged SV model can be expressed as**

$\begin{aligned} y_t&=\beta_0+\beta_1 y_{t-1}+\exp(h_t/2)\epsilon_t \quad \text{for}\quad 1\le t\le T,\\ h_{t+1}&=\mu+\phi(h_t-\mu)+\sigma \eta_t\quad \text{for} \quad 0\le t\le T, \quad h_0\sim N(\mu, \sigma^2/(1-\phi^2)),\\ \eta_t&\sim N(0,1)\\ \epsilon_t|\eta_t&\sim t_{\nu}(\rho \eta_t ,1). \end{aligned}$

**Here** $\nu$ **is the degrees of freedom parameter (unknown).**

**Implement this model in JAGS or Stan on the first 3 months of USD/EUR
data from the dataset.**

**Explain how did you choose priors for all parameters. Explain how did
you take into account the days without observation in your model.**

**Fit the model, do convergence diagnostics, print out the summary of
the results, and discuss them.**

**Make sure that the Effective Sample Size is at least 1000 for all 6
hyperparameters (you need to choose burn-in and number of steps
appropriately for this).**

$$
y_t \sim \mathcal{N} (\beta_0+\beta_1y_{t-1}, \exp(h_t))
$$

```{r}
#model in BUGS syntax
model2_string <- "model {
  # prior
  beta0 ~ dnorm(beta.mu.0, beta.tau.0)
  beta1 ~ dnorm(beta.mu.1, beta.tau.1)
  mu ~ dnorm(mu.mu,mu.tau)
  trans_phi ~ dbeta(20,1.5)
  phi <- 2*trans_phi-1
  tau ~ dgamma(sigma.a,sigma.b)
  sigma <- pow(tau,-2)
  rho ~ dunif(-1,1)
  k ~ dchisq(1)
  
  # Likelihood
  h[1] ~ dnorm(mu, (1-phi^2)*tau)
  #The evolution of the hidden states x according to the model
  for(i in 2:(n+1)) {
    h[i] ~ dnorm(mu+phi*(h[i-1]-mu), tau) 
  }
  
    
  z.inv[1] ~ dgamma(k/2,k/2)
  z[1] <- pow(z.inv[1],-1)
  
  y.mu[1] <- rho/sigma*sqrt(z[1])*exp(h[1]/2)*(h[2]-mu-phi*(h[1]-mu))+beta0
  y.var[1] <- z[1]*exp(h[1])*(1-rho*rho)
  y[1] ~ dnorm(y.mu[1], pow(y.var[1],-1)) 
  for(i in 2:n) {
    z.inv[i] ~ dgamma(k/2,k/2)
    z[i] <- 1/z.inv[i]

    y.mu[i] <- rho/sigma*sqrt(z[i])*exp(h[i]/2)*(h[i+1]-mu-phi*(h[i]-mu)) + beta0 + beta1*y[i-1]
    y.var[i] <- z[i]*exp(h[i])*(1-pow(rho,2)) + beta1^2*y.var[i-1]
    y[i] ~ dnorm(y.mu[i], pow(y.var[i],-1))
  }
  


}"
```

```{r}
#model in BUGS syntax 14:17
model2_string <- "model {
  # prior
  beta0 ~ dnorm(beta.mu.0, beta.tau.0)
  beta1 ~ dnorm(beta.mu.1, beta.tau.1)
  mu ~ dnorm(mu.mu,mu.tau)
  trans_phi ~ dbeta(18,1.5)
  phi <- 2*trans_phi-1
  tau ~ dgamma(sigma.a,sigma.b)
  sigma <- pow(tau,-2)
  rho ~ dunif(-1,1)
  k ~ dchisq(8) 
  
  # Likelihood
  h[1] ~ dnorm(mu, (1-phi^2)*tau)
  #The evolution of the hidden states x according to the model
  for(i in 2:(n+1)) {
    eta[i] ~ dnorm(0,1)
    h[i] ~ dnorm(mu+phi*(h[i-1]-mu), tau)
  }
  
  
  y.mu[1] <- 0.5
  y.tau[1] <- exp(-h[1])
  y[1] ~ dt(y.mu[1], y.tau[1], k) 
  for(i in 2:n) {
    y.mu[i] <- rho*eta[i]*exp(h[i]/2) + beta0 + beta1*y[i-1]
    y.tau[i] <- exp(-h[i])
    y[i] ~ dt(y.mu[i], y.tau[i],k)
  }
  


}"

eur2usd.data2 <- list(n=n,y=y,
                     beta.mu.0=0,beta.tau.0=0.001,
                     beta.mu.1=0,beta.tau.1=0.001,
                     mu.mu=0,mu.tau=0.001,
                     sigma.a=1,sigma.b=1)


num.chains <- 5
# Compile and run the model
model2 <- jags.model(file=textConnection(model2_string), 
                        data=eur2usd.data2,
                        n.chains=num.chains)
# Burnin for 20000 samples
update(model2,10000)

```

```{r}
# Running the model, monitoring the variable theta
res.model2=coda.samples(model2,variable.names=c("beta0", "beta1", "mu", "phi", "sigma", "rho", "k"), n.iter=50000,progress.bar="none")

#Setting margins to be small
summary(res.model2); effectiveSize(res.model2);
plot(res.model2)
```
```{r}
par(mfrow = c(3,2))
acf(res.model2[[1]][,"beta0"],lag.max=1000,main="ACF for beta0")
acf(res.model2[[1]][,"beta1"],lag.max=1000,main="ACF for beta1")
acf(res.model2[[1]][,"mu"],lag.max=5000,main="ACF for mu")
acf(res.model2[[1]][,"phi"],lag.max=1000,main="ACF for phi")
acf(res.model2[[1]][,"rho"],lag.max=1000,main="ACF for rho")
acf(res.model2[[1]][,"sigma"],lag.max=1000,main="ACF for sigma")

```


Explanation: (Write your explanation here)

**c)[10 marks]**

**Perform posterior predictive checks on both models a) and b). Explain
how did you choose the test functions.**

**Discuss the results.** 
### Posterior predictive checks for (a)

```{r}
#model in BUGS syntax

model1_string <- "model {
  # prior
  beta0 ~ dnorm(beta.mu.0, beta.tau.0)
  beta1 ~ dnorm(beta.mu.1, beta.tau.1)
  mu ~ dnorm(mu.mu,mu.tau)
  trans_phi ~ dbeta(20,1.5)
  phi <- 2*trans_phi-1
  tau ~ dgamma(sigma.a,sigma.b) 
  sigma <- pow(tau,-1/2)
  trans_rho ~ dbeta(3,5)
  rho <- 2*trans_phi-1
  # Likelihood
  h[1] ~ dnorm(mu, (1-phi^2)*tau)
  #The evolution of the hidden states x according to the model
  for(i in 2:(n+1)) {
    h[i] ~ dnorm(mu+phi*(h[i-1]-mu), tau) 
  }
  
  y[1] ~ dnorm(mean.y0,prec.y0)
  y_rep[1] ~ dnorm(mean.y0,prec.y0)
  y.var[1] <- prec.y0^(-1)
  for(i in 2:n) {
    y.mu[i] <- rho/sigma*exp(h[i]/2)*(h[i+1]-mu-phi*(h[i]-mu))+beta0 + beta1*y[i-1]
    y.var[i] <- exp(h[i])*(1-rho^2) 
    y[i] ~ dnorm(y.mu[i], pow(y.var[i],-1))
    y_rep[i] ~ dnorm(y.mu[i], pow(y.var[i],-1))
  }
  


}"
```

```{r}
num.chains <- 5
# Compile and run the model
model1_check <- jags.model(file=textConnection(model1_string), 
                        data=eur2usd.data,
                        n.chains=num.chains)
# Burnin for 20000 samples
update(model1_check,50000)
```

```{r}
# Running the model, monitoring the variable theta
res.model1_check=coda.samples(model1_check,variable.names=c("y_rep"), n.iter=50000,progress.bar="none")

#Setting margins to be small
#summary(res.model1_check); effectiveSize(res.model1_check);

```

```{r}
y1rep=res.model1_check[[1]]
y1repmin=apply(y1rep,1,min)
hist(y1repmin,col="gray40") 
abline(v=min(y,na.rm = TRUE),col="red",lwd=2)
```

```{r}

y1repmax=apply(y1rep,1,max)
hist(y1repmax,col="gray40") 
abline(v=max(y,na.rm = TRUE),col="red",lwd=2)

```

```{r}

y1rep=res.model1_check[[1]]
y1repmedian=apply(y1rep,1,median)
hist(y1repmedian,col="gray40") 
abline(v=median(y,na.rm = TRUE),col="red",lwd=2)

```
```{r}

y1rep=res.model1_check[[1]]
y1repmean=apply(y1rep,1,mean)
hist(y1repmean,col="gray40") 
abline(v=mean(y,na.rm = TRUE),col="red",lwd=2)

```
```{r}
## Plot posterior predictive 
for (i in length(y)){
  y1rep[]
}

```

### Posterior check for (b)
```{r}
#model in BUGS syntax 14:17
model2_string <- "model {
  # prior
  beta0 ~ dnorm(beta.mu.0, beta.tau.0)
  beta1 ~ dnorm(beta.mu.1, beta.tau.1)
  mu ~ dnorm(mu.mu,mu.tau)
  trans_phi ~ dbeta(18,1.5)
  phi <- 2*trans_phi-1
  tau ~ dgamma(sigma.a,sigma.b)
  sigma <- pow(tau,-2)
  rho ~ dunif(-1,1)
  k ~ dchisq(8) 
  
  # Likelihood
  h[1] ~ dnorm(mu, (1-phi^2)*tau)
  #The evolution of the hidden states x according to the model
  for(i in 2:(n+1)) {
    eta[i] ~ dnorm(0,1)
    h[i] ~ dnorm(mu+phi*(h[i-1]-mu), tau)
  }
  
  
  y.mu[1] <- 0.5
  y.tau[1] <- exp(-h[1])
  y[1] ~ dt(y.mu[1], y.tau[1], k) 
  for(i in 2:n) {
    y.mu[i] <- rho*eta[i]*exp(h[i]/2) + beta0 + beta1*y[i-1]
    y.tau[i] <- exp(-h[i])
    y[i] ~ dt(y.mu[i], y.tau[i], k)
    y_rep[i] ~ dt(y.mu[i], y.tau[i], k)
  }
  


}"

eur2usd.data2 <- list(n=n,y=y,
                     beta.mu.0=0,beta.tau.0=0.001,
                     beta.mu.1=0,beta.tau.1=0.001,
                     mu.mu=0,mu.tau=0.001,
                     sigma.a=1,sigma.b=1)


num.chains <- 5
# Compile and run the model
model2_check <- jags.model(file=textConnection(model2_string), 
                        data=eur2usd.data2,
                        n.chains=num.chains)
# Burnin for 50000 samples
update(model2_check,50000)

```

```{r}
# Running the model, monitoring the variable theta
res.model2_check=coda.samples(model2_check,variable.names=c("y_rep"), n.iter=50000,progress.bar="none")

```

```{r}
y2rep=res.model2_check[[1]]
y2repmin=apply(y2rep,1,min)
hist(y2repmin,col="gray40",xlim=c(-50,2)) 
abline(v=min(y,na.rm = TRUE),col="red",lwd=2)
```
```{r}
y2repmax=apply(y2rep,1,max)
hist(y2repmax,col="gray40") 
abline(v=max(y,na.rm = TRUE),col="red",lwd=2)
```
```{r}
y2repmean=apply(yrep,1,mean)
hist(y2repmean,col="gray40") 
abline(v=mean(y,na.rm = TRUE),col="red",lwd=2)
```
```{r}

```


Explanation: (Write your explanation here)

**d)[10 marks]**

**Based on your models a) and b), plot the posterior predictive
densities of the USD/EUR rate on the dates 2000-04-03, 2020-04-04 and
2020-04-05 (the next 3 days after the period considered). Compute the
posterior means and 95% credible intervals. Discuss the results.**

Explanation: (Write your explanation here)

**e)[10 marks]**

**In this question, we are going to look use a multivariate stochastic
volatility model with leverage to study the USD/EUR and GBP/EUR exchange
rates jointly. The model is described as follows,**

$\begin{aligned}\boldsymbol{y}_t&=\boldsymbol{\beta}_0+\boldsymbol{\beta}_1 \boldsymbol{y}_{t-1}+\exp(h_t/2)\boldsymbol{\epsilon}_t \quad \text{for}\quad 1\le t\le T,\\ \boldsymbol{h}_{t+1}&=\boldsymbol{\phi}(\boldsymbol{h}_t)+\boldsymbol{\eta}_t\quad \text{for} \quad 0\le t\le T, \quad h_0\sim N(0, I),\\ (\epsilon_t,\eta_t)&\sim N\left(0, \Sigma\right).\end{aligned}$

**Here I denotes the 2 x 2 identity matrix,**
$\boldsymbol{y}_t, \boldsymbol{\beta}_0, \boldsymbol{h}_t, \boldsymbol{\eta}_t, \boldsymbol{\epsilon}_t$
**are 2 dimensional vectors,** $\boldsymbol{\beta}_1$ **and**
$\boldsymbol{\phi}$ **are 2 x 2 matrices,** $\boldsymbol{\Sigma}$ **is a
4 x 4 covariance matrix. At each time step** $t$**, the two components
of** $y_t$ **will be used to model the USD/EUR and GBP/EUR exchange
rates, respectively.**

**Implement this model in JAGS or Stan.**

**Discuss your choices for priors for every parameter [Hint: you can use
Wishart or scaled Wishart priors for\*\*** $\boldsymbol{\Sigma}$,
\*\*see
<https://www.stats.ox.ac.uk/~nicholls/MScMCMC15/jags_user_manual.pdf> ,
<https://mc-stan.org/docs/2_19/functions-reference/wishart-distribution.html>].

**Fit the model, do convergence diagnostics, print out the summary of
the results, and discuss them.**

```{r}

```

Explanation: (Write your explanation here)

![](nba.jpg)

**Problem 2 - NBA data**

**In this problem, we are going to construct a predictive model for NBA
games.**

**We start by loading the dataset.**

```{r}
games<-read.csv("games.csv")
teams<-read.csv("teams.csv")
```

**games.csv contains the information about games such as GAME_DATE,
SEASON, HOME_TEAM_ID, VISITOR_TEAM_ID, PTS_home (final score for home
team) and PTS_away (final score for away team).**

**teams.csv contains the names of each team, i.e. the names
corresponding to each team ID.**

**We are going to fit some Bayesian linear regression models on the
scores of each team.**

**You can use either INLA, JAGS or Stan.**

**a)[10 marks]**

**The dataset contains data from 20 seasons, but we are going to focus
on only one, the 2021 season.\
Please only keep games where SEASON is 2021 in the dataset, and remove
all other seasons.\
Please order the games according to the date of occurrence (they are not
ordered like that in the dataset).**

**The scores are going to be assumed to follow a linear Gaussian
model,**

$$S_g^{H}\sim N(\mu_{g}^{H},\sigma^2), \quad S_g^{A}\sim N(\mu_{g}^{A}, \sigma^2).$$

**Here** $S_g^H$ **denotes the final score of the home team in game**
$g$**, and** $S^A_g$ **denotes the final score of the away team in
game** $g$**.**

**Note that the true scores can only take non-negative integer values,
so the Gaussian distribution is not perfect, but it can still be used
nevertheless.**

**The means for the scores are going to be modeled as a combination of
three terms: attacking strength, defending ability, and whether the team
is playing at home, or away. For each team, we denote their attacking
strength parameter by** $a_{team}$**, their defending strength parameter
by** $d_{team}$**, and the effect of playing at home as** $h$**. This
quantifies the effect of playing at home on the expected number of goals
scored. Our model is the following (**$\mu_g^{H}$ **is for the goals
scored by the home team, and is** $\mu_g^{A}$ **is for the away team):**

$\begin{aligned} \mu_{g}^{H}&= \beta_0+a_{home.team}+d_{away.team}+h\\ \mu_{g}^{A}&= \beta_0+a_{away.team}+d_{home.team} \end{aligned}$

**Implement this model. Select your own prior distributions for the
parameters, and discuss the reason for using those priors.**

**Obtain the summary statistics for the posterior distribution of the
model parameters.**

**Evaluate the root mean square error (RMSE) of your posterior means
versus the true scores.**

**Interpret the results.**

```{r}
# Create dataset
games_data <- games %>%
  filter(SEASON %in% 2021) %>%
  select(GAME_DATE_EST,HOME_TEAM_ID,VISITOR_TEAM_ID,PTS_home,PTS_away) %>%
  arrange(GAME_DATE_EST) %>% 
  group_by(HOME_TEAM_ID,VISITOR_TEAM_ID)

plot(density(games_data$PTS_home),type="l")
lines(density(games_data$PTS_away),type="l",col="red")
```

```{r}

y2=c(games_data$PTS_home, games_data$PTS_away)
G=nrow(games_data)

HT_char=as.character(games_data$HOME_TEAM_ID)
AT_char=as.character(games_data$VISITOR_TEAM_ID)
attack=as.factor(c(HT_char,AT_char))
defense=as.factor(c(AT_char,HT_char))
h=c(rep(1,G),rep(0,G))

```

```{r}
prior.prec <- list(prec=list(prior = "loggamma", param = c(0.1, 0.1)))
prior.fixed <- list(mean.intercept = mean(y2), prec.intercept = 0.01,
                    mean = 0, prec = 0.01)
data=data.frame(y2,attack,defense,h)
m.gaussian=inla(formula=y2~1+attack+defense+h,
                data=data,family="gaussian",
                control.family=list(hyper=prior.prec),
                control.fixed=prior.fixed,
                control.compute = list(dic = T,config = TRUE,
                                       return.marginals.predictor=TRUE))
summary(m.gaussian)
```

```{r}
fittedvalues1 <- m.gaussian$summary.fitted.values

```

Explanation: (Write your explanation here)

**b)[10 marks] In part a), the model assumed that the home effect is the
same for each team. In this part, we consider a team-specific home
effect** $h_{home.team}$,

$\begin{aligned} \mu_{g}^{H}&= \beta_0+a_{home.team}+d_{away.team}+h_{home.team}\\ \mu_{g}^{A}&= \beta_0+a_{away.team}+d_{home.team} \end{aligned}$

**Implement this model. Select your own prior distributions for the
parameters, and discuss the reason for using those priors.**

**Obtain the summary statistics for the posterior distribution of the
model parameters.**

**Evaluate the root mean square error (RMSE) of your posterior means
versus the true scores.**

**Interpret the results.**

```{r}

h.team=c(as.character(games_data$HOME_TEAM_ID),rep(0,G))
prior.prec <- list(prec=list(prior = "loggamma", param = c(0.1, 0.1)))
prior.fixed <- list(mean.intercept = mean(y2), prec.intercept = 0.001,
                   mean = 0, prec = 0.01)
data=data.frame(y2,attack,defense,h.team)
m2.gaussian=inla(formula=y2~1+attack+defense+h.team,
                data=data,family="gaussian",
                control.family=list(hyper=prior.prec),
                control.fixed=prior.fixed,
                control.compute = list(dic = T,config = TRUE,
                                       return.marginals.predictor=TRUE))
summary(m2.gaussian)

```

Explanation: (Write your explanation here)

**c)[10 marks] Propose an improved linear model using the information in
the dataset before the game (you cannot use any information in the same
row as the game, as this is only available after the game). Hint: you
can try incorporating running averages of some covariates specific to
each team, by doing some pre-processing.**

**Implement your model. Select your own prior distributions for the
parameters, and discuss the reason for using those priors.**

**Obtain the summary statistics for the posterior distribution of the
model parameters.**

**Evaluate the root mean square error (RMSE) of your posterior means
versus the true scores.**

**Interpret the results.**

```{r}

```

Explanation: (Write your explanation here)

**d)[10 marks] Perform posterior predictive checks on all 3 models a),
b), and c). Explain how did you choose the test functions.**

**Discuss the results.**

```{r}

```

Explanation: (Write your explanation here)

**e)[10 marks] In the previous questions, we were assuming a model of
the
form.**$$S_g^{H}\sim N(\mu_{g}^{H},\sigma^2), \quad S_g^{A}\sim N(\mu_{g}^{A}, \sigma^2).$$**It
is natural to model these two results jointly with a multivariate
normal,**

$$(S_g^{H}, S_g^{A})\sim N\left(\left(\begin{matrix}\mu_{g}^{H}\\\mu_{g}^{A}\end{matrix}\right),\Sigma\right),$$

**where** $\Sigma$ **is a 2 times 2 covariance matrix.**

**Implement such a model. The definition of** $\mu_g^{H}$ **and**
$\mu_g^{A}$ **can be either one of a), b), or c), you just need to
implement one of them.**

**Explain how did you choose the prior on** $\Sigma$ **[Hint: you can
use a Wishart prior, or express this a product of diagonal and
correlation matrices and put priors on those terms].**

**Obtain the summary statistics for the posterior distribution of the
model parameters.**

**Evaluate the root mean square error (RMSE) of your posterior means
versus the true scores.**

**Interpret the results.**

```{r}

```

Explanation: (Write your explanation here)
