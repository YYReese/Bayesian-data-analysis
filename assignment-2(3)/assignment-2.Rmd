---
editor_options:
  markdown:
    wrap: 72
output:
  html_document:
    df_print: paged
---

**University of Edinburgh**

**School of Mathematics**

**Bayesian Data Analysis, 2022/2023, Semester 2**

**Assignment 2**

**IMPORTANT INFORMATION ABOUT THE ASSIGNMENT**



```{r}
rm(list = ls(all = TRUE))
#Do not delete this!
#It clears all variables to ensure reproducibility
```

![](car_insurance.jpg)

**Problem 1**

**In this problem, we study a dataset about car insurance.** **This data
set is based on one-year vehicle insurance policies taken out in 2004 or
2005. In total, there are 67856 policies, of which 4624 have claims.**

```{r}
require(insuranceData)
require(dplyr)
require(knitr)
require(dummies)
require(Metrics)
require(distributions3)
require(INLA)
data(dataCar)

#The first 6 rows of the dataframe
print.data.frame(dataCar[1:6,])

```

**Description of the columns.**

**veh_value: vehicle value in \$10000s**

**exposure: maximum portion of the vehicle value the insurer may need to
pay out in case of an incident**

**claimcst0: claim amount (0 if no claim)**

**clm: whether there was a claim during the 1 year duration**

**numclaims: number of claims during the 1 year duration**

**veh_body types: BUS = bus CONVT = convertible COUPE = coupe HBACK =
hatchback HDTOP = hardtop MCARA = motorized caravan MIBUS = minibus
PANVN = panel van RDSTR = roadster SEDAN = sedan STNWG = station wagon
TRUCK = truck UTE = utility**

**gender: F- female, M - male\
\
area: a factor with levels A,B,C,D,E, F**

**agecat: age category, 1 (youngest), 2, 3, 4, 5, 6**

**You can use either JAGS, Stan, or INLA for this question.**

**a)[10 marks] Fit a Bayesian logistic regression model on the dataset
dataCar with**

-   **clm as response,**

-   **a link function of your choice,**

-   **using veh_value, exposure, veh_body, veh_age, gender, area, and
    agecat as covariates (you can use categorical covariates by
    converting integers to factors if appropriate).**

**Center and scale the non-categorical covariates.**

**Choose your own prior distributions (do not use default priors), and
explain the rationale your prior choices, and ensure that the posterior
is not too sensitive to your prior choice [Hint: look at the induced
prior on the linear predictor and on the response.]**

**Compute the posterior means of the model parameters, and discuss the
results.**

```{r}
# Define the functions 
odds <- function(x) x/(1-x) # odds ratio function
logit <- function(x) log(x/(1-x))
ilogit <- function(x){ 1/(1+exp(-x))} #Inverse logistic function
```

In this part, we standardise two non-categorical covariates, veh_value and exposure, and convert veh_age to categorical variable.
```{r}
# Data preprocessing 
car.data.scl <- dataCar %>% 
  select(veh_value, exposure,veh_body, 
         veh_age, gender, area, agecat, clm,numclaims) %>% 
  mutate(veh_value=scale(veh_value), 
         exposure=scale(exposure))

####Car data with categorical covariates - INLA ####
car.inla.data <- car.data.scl %>% 
  select(veh_value, exposure,veh_body, 
         veh_age, gender, area, agecat, clm, numclaims) %>%
  mutate(veh_body=as.factor(veh_body),area=as.factor(area),
         agecat=as.factor(agecat),veh_age=as.factor(veh_age))
```

### Data exploration
In the following, we compute the odds ratio of different groups of the data which might be useful for prior choices. 
```{r}
odds.body <- c()
for (i in 1:13){
  body <- unique(car.inla.data$veh_body)[i]
  df.body <- car.inla.data %>% filter(veh_body %in% c(body)) 
  odds.body[i] <- odds(sum(df.body$clm == 1)/nrow(df.body))
}
body.ratio <- max(odds.body)/min(odds.body)
body.ratio 

odds.gender <- c()
df.m <- car.inla.data %>% filter(gender %in% c("M"))
df.f <- car.inla.data %>% filter(gender %in% c("F"))
odds.gender[1] <- odds(sum(df.m$clm == 1)/nrow(df.m))
odds.gender[2] <- odds(sum(df.f$clm == 1)/nrow(df.f))
gender.ratio <- max(odds.gender)/min(odds.gender)
gender.ratio

odds.area <- c()
for (i in 1:6){
  a <- unique(car.inla.data$area)[i]
  df.area <- car.inla.data %>% filter(area %in% c(a)) 
  odds.area[i] <- odds(sum(df.area$clm == 1)/nrow(df.area))
}
area.ratio <- max(odds.area)/min(odds.area)
area.ratio

odds.agecat <- c()
for (i in 1:6){
  age <- unique(car.inla.data$agecat)[i]
  df.agecat <- car.inla.data %>% filter(agecat %in% c(age)) 
  odds.agecat[i] <- odds(sum(df.agecat$clm == 1)/nrow(df.agecat))
}
agecat.ratio <- max(odds.agecat)/min(odds.agecat)
agecat.ratio


odds.v_age <- c()
for (i in 1:4){
  v_age <- unique(car.inla.data$veh_age)[i]
  df.v_age <- car.inla.data %>% filter(veh_age %in% c(v_age)) 
  odds.v_age[i] <- odds(sum(df.v_age$clm == 1)/nrow(df.v_age))
}
v_age.ratio <- max(odds.v_age)/min(odds.v_age)
v_age.ratio

```
From the results, we can see that for veh_body, the odds ratio of one group can be at most $6$ times lager than that of another group. For gender, the odds ratio of female is $0.017$ times lager than that of male. For area, the odds ratio of one group is at most $1.314$ times larger than that of another group. For agecat, the odds ratio of one group is at most $1.601$ times larger than that of another group. For veh_age, the odds ratio of one group is at most $1.239$ times larger than that of another group. Using the information, we set the priors for the regression coefficients of linear predictors as follows.
\begin{itemize}
  \item For veh_body: we want $e^{\beta_3}$ to have large mass at around $6$, so it is reasonable to set prior $\mathcal{N}(0,\sigma_3^2)$ where $\sigma_3=\ln 6$. 
  \item For gender: we want $e^{\beta_5}$ to have large mass at around $1.02$, so we may like to put prior $\mathcal{N}(0,\sigma_5^2)$ where $\sigma_5=\ln 1.02$. However, this looks a bit informative, so we let $\sigma_5=1$ instead. It cause no problem to change the prior to make it vaguer. 
  \item For area: we want $e^{\beta_6}$ to have large mass at around $1.31$, so it is reasonable to set prior $\mathcal{N}(0,\sigma_6^2)$ where $\sigma_6=\ln 1.31$. 
  \item For agecat: we want $e^{\beta_7}$ to have large mass at around $1.6$, so it is reasonable to set prior $\mathcal{N}(0,\sigma_7^2)$ where $\sigma_7=\ln 1.6$. 
  \item For veh_age: we want $e^{\beta_4}$ to have large mass at around $1.24$, so it is reasonable to set prior $\mathcal{N}(0,\sigma_6^2)$ where $\sigma_6=\ln 1.24$. 
  \item For veh_value: we want $e^{\beta_1 x_1}$ to have large mass at around $5$. Having checked the density of scaled value of veh_value variable, we think it takes values mostly between $0$ to $5$. Therefore, it is sensible to set prior $\mathcal{N}(0,\sigma_1^2)$ where $\sigma_1=\ln 5/5$. 
  \item For exposure: we want $e^{\beta_2 x_2}$ to have large mass at around $1.5$. Having check the density of scaled value of exposure, we found it takes values mostly within $-1$ to $1$. Therefore, it is sensible to set prior $\mathcal{N}(0,\sigma_2^2)$ where $\sigma_2=\ln 1.5/1=\ln 1.5$. 
  For the intercept $beta_0$: we want to ensure that the inverse logit of $30$ times $|\beta_0|$ can be in the interval $(-0.05,0.95)$. (Here $30$ is roughly the exponential of all the covariates' effects, i.e. $50\approx e^{\beta_1 x_1 \beta_2 x_2 ...\beta_7}$.) Hence, we come up with the prior $\mathcal{N}(0, \sigma_0)$ for $\beta_0$ where $\sigma_0=log(6)$.
\end{itemize}

Now let us see the induced prior on the probability parameter in Bernoulli distribution $p_i$. We simulate $1000$ points for each beta. Assume we have each all non-categorical covariates equal to 1 (after scaling) and randomly choose a group for all the categorical variables. Then we can compute the responses. After that, the inverse logit of the responses can be computed so that we can plot the density of the probability parameter in Bernoulli distribution, $p_i$. From the following plot, we can see the induced prior on $\mu$ looks okay. We have also tried different values of $x_1,x_2,...$ and the results showed no problem, so we continue with our choices of priors.
```{r}
beta0 <- rnorm(1000,sd=1.79)
beta1 <- rnorm(1000,sd=log(5)/5)
beta2 <- rnorm(1000,sd=log(1.5))
beta3 <- rnorm(1000,sd=log(6))
beta4 <- rnorm(1000,sd=log(1.24))
beta5 <- rnorm(1000,sd=log(1.02))
beta6 <- rnorm(1000,sd=log(1.31))
beta7 <- rnorm(1000,sd=log(1.6))
x <- ilogit( beta0+beta1+beta2+beta3+beta4+beta5+beta6+beta7)
plot(density(x),main="density of the induced prior on the parameter p in Bernoulli")
```


### Multiple regression in INLA
The logistic regression model using the required covariates was implemented in INLA. In previous part, we have standerdise the non-categorical covariates so that the mean is 1 and standered deviation is $1$ for each non-categorical covariate. For categorical covariates, INLA will automatically create $k-1$ indicators, where $k$ is the number of catogories for certain covariate. The link function is chosen to be the logit function in our model.

The results are shown below. The mean of the regression coefficients can be found in the first column of the summary results.
```{r}
#Priors for the regression coefficients.
prior.beta.logit <- list(mean.intercept = 0, prec.intercept =  0.31,
                    mean =0, 
                   prec = list(veh_value=5,exposure=6,
                               veh_body=0.3,veh_age=21,
                               gender=1,area=13.7,agecat=4.5))

#Fitting the model in INLA
#"control.fixed=prior.beta" sets regression coeff. priors.
results.car.logit <- inla(formula = clm ~ veh_value+exposure+veh_body+
               veh_age+gender+area+agecat, family="binomial",Ntrials=1,
             control.family=list(link="logit"),
             data=car.inla.data, control.fixed=prior.beta.logit,
             control.predictor = list(compute = TRUE,link=1),
             control.compute=list(config=TRUE,cpo=TRUE, dic=TRUE))
summary(results.car.logit)

```

As we created the reference group to be veh_body=BUS, gender=F, area=1,agecat=1, veh_age=1, the intercept $\beta_0$ reflects the scaled log odds ratio of this reference group. We can compute the mean of $p$ of the reference group, which is $\text{ilogit}(-1.351)=0.2057$. Hence, for the reference group, the probability of a claim is around $0.2$. For categorical covariate veh_body, the regression coefficient for each group is negative, indicating that all other vehicle categories have the probability of a claim lower than the BUS, and thus BUS is the one that most probable to have a claim. In addition, we see the regression coefficient for motorized caravan is the smallest among all the groups, which indicates that when other factors remain the same, motorized caravan is most likely to have a claim (apart from BUS). Similarly, we found the convertible vehicle is the least likely to have a claim, as the regression coefficient is the smallest among all the vehicle categories. For the covariate veh_age, we observe a increase trend in the regression coefficients. Therefore, we may believe that as the vehicle get older, the probability of a claim decreases. For the covariate gender, the regression coefficient is $-0.019$ for female. After taking the inverse logit, we can say the female have roughly $1-\text{ilogit}(-0.019)=50.47\%$ less chance to claim than male. For covariate area, the regression coefficients for area B and F are positive, showing higher probabilities of a claim compared with the area A. The regression coefficients for area D and E are negative, showing lower probabilities of a claim compared with the area A. For the covariate agecat, we see the younger groups of people seem to have a larger probability to claim. 

For the non-categorical variable, we need to transfer the coefficients back to the original scale to better interpret the data. The transformed values are shown below. The odds ratios of the predictors can be calculated by exponentiating the estimated coefficients. For the vehicle value, we expect $4.57\%$ increase in the odds ratio of having a claim for every unit increase in the veh_value. Similarly, we expect $16.92\%$ increase in the odds ratio of having a claim for one unit increase in exposure, assuming all other predictors are fixed.

```{r}
cat("Regression coefficient for veh_value after transformation to the original scale: \n",
results.car.logit$summary.fixed$mean[2] * sd(dataCar$veh_value))
cat("Take the exponential to get the odds ratio: \n",
exp(results.car.logit$summary.fixed$mean[2] * sd(dataCar$veh_value)))

cat("Regression coefficient for exposure after transformation to the original scale: \n",
results.car.logit$summary.fixed$mean[3] * sd(dataCar$exposure))
cat("Take the exponential to get the odds ratio: \n",
exp(results.car.logit$summary.fixed$mean[3] * sd(dataCar$exposure)))
```

Now, we can compute the posterior mean of the model parameter ($p$) in Bernoulli distribution across different groups.

```{r}
fittedvaluesm.A <- results.car.logit$summary.fitted.values$mean
res.A <- car.data.scl %>% mutate(prob = ilogit(fittedvaluesm.A))
```

These are the posterior means of $p$ for different vehicle bodies. We see BUS has the highest claim probability while the convertible vehicle has the least probability to claim. Basically, we can get similar conclusion as the discussion in the previous part where the regression coefficients were discussed.
```{r}
res.A %>% 
  group_by(veh_body) %>%
  summarise(p.mu=mean(prob))
```

Female seems to have a slightly higher probability to claim. The difference is not significant though.
```{r}
res.A %>% 
  group_by(gender) %>%
  summarise(p.mu=mean(prob))
```
The oldest group of vehicle tends to be less likely to have a claim.
```{r}
res.A %>% 
  group_by(veh_age) %>%
  summarise(p.mu=mean(prob))
```

The younger groups of people tends to have a higher probability to claim.
```{r}
res.A %>% 
  group_by(agecat) %>%
  summarise(p.mu=mean(prob))
```

Area F is the area where the probaility of a claim is the largest across different area. Area D is where the probaility of a claim is the smallest.
```{r}
res.A %>% 
  group_by(area) %>%
  summarise(p.mu=mean(prob))
```


**b)[10 marks] Fit a Bayesian Poisson regression model on numclaims as
response with**

-   **log link function,**

-   **using veh_value, exposure, veh_body, veh_age, gender, area, and
    agecat as covariates.**

**Center and scale the non-categorical covariates.**

**Choose your own prior distributions (do not use default priors), and
explain the rationale your prior choices, and ensure that the posterior
is not too sensitive to your prior choice [Hint: look at the induced
prior on the linear predictor and the response.]**

**Compute the posterior means of the model parameters, and discuss the
results.**

We adopt a similar strategy for choosing priors as in part (a). However, here we look at the probability of each group instead of the odds ratio.
```{r}
mu.body <- c()
for (i in 1:13){
  body <- unique(car.inla.data$veh_body)[i]
  df.body <- car.inla.data %>% filter(veh_body %in% c(body)) 
  mu.body[i] <- mean(df.body$numclaims)
}
body.ratio <- max(mu.body)/min(mu.body)
body.ratio 

mu.gender <- c()
df.m <- car.inla.data %>% filter(gender %in% c("M"))
df.f <- car.inla.data %>% filter(gender %in% c("F"))
mu.gender[1] <- mean(df.m$numclaims)
mu.gender[2] <- mean(df.f$numclaims)
gender.ratio <- max(mu.gender)/min(mu.gender)
gender.ratio

mu.area <- c()
for (i in 1:6){
  a <- unique(car.inla.data$area)[i]
  df.area <- car.inla.data %>% filter(area %in% c(a)) 
  mu.area[i] <- mean(df.area$numclaims)
}
area.ratio <- max(mu.area)/min(mu.area)
area.ratio

mu.agecat <- c()
for (i in 1:6){
  age <- unique(car.inla.data$agecat)[i]
  df.agecat <- car.inla.data %>% filter(agecat %in% c(age)) 
  mu.agecat[i] <- mean(df.agecat$numclaims)
}
agecat.ratio <- max(mu.agecat)/min(mu.agecat)
agecat.ratio


mu.v_age <- c()
for (i in 1:4){
  v_age <- unique(car.inla.data$veh_age)[i]
  df.v_age <- car.inla.data %>% filter(veh_age %in% c(v_age)) 
  mu.v_age[i] <- mean(df.v_age$numclaims)
}
v_age.ratio <- max(mu.v_age)/min(mu.v_age)
v_age.ratio
```

For the choices of priors for the poisson regression, we first try to put $\mathcal{N}(0,1)$ on the intercept and $\mathcal{N}(0,0.1)$ on the other regression coefficients. The density of induced prior on model parameter $\mu$ in Poisson distribution is simulated below. We can see there is a hugh amount of probability mass between $0$ to $10$, as desired. We will continue with the choices of priors.
```{r}
beta0 <- rnorm(1000,sd=1)
beta1 <- rnorm(1000,sd=0.2)
beta2 <- rnorm(1000,sd=0.2)
beta3 <- rnorm(1000,sd=0.2)
beta4 <- rnorm(1000,sd=0.2)
beta5 <- rnorm(1000,sd=0.2)
beta6 <- rnorm(1000,sd=0.2)
beta7 <- rnorm(1000,sd=0.2)
x <- exp( beta0+0.4*beta1+beta2+beta3+beta4+beta5+beta6+beta7)
plot(density(x),main="density of the induced prior on the parameter mu in Poisson")
```

We fit the regression model in INLA using log as the link function. The results are summarised as follows.

```{r}

#Priors for the regression coefficients.
prior.beta.log <- list(mean.intercept = 0, prec.intercept =  1,
                    mean =0, 
                   prec = 25)
#Fitting the model in INLA
#"control.fixed=prior.beta" sets regression coeff. priors.
results.car.log <- inla(formula = numclaims ~ veh_value+exposure+veh_body+
               veh_age+gender+area+agecat, family="poisson",
             control.family=list(link="log"),
             data=car.inla.data, control.fixed=prior.beta.log,
             control.predictor = list(compute = TRUE,link=1),
             control.compute=list(config=TRUE,cpo=TRUE, dic=TRUE))
summary(results.car.log)
```
For interpreting the coefficients we can exponentiate them. For the intercept, $e^{-2.451}=0.086$ represents the average number of claims of the reference category (i.e. the group with veh_body=BUS , veh_age=1, gender=M, area=A and agecat=1). The exponential function applied to the coefficients of the categorical factors provides the relative rate of the number of claims for different categories. For example, looking at the group veh_body=CONVT, $e^{-0.131}=0.877$, thus there is a $12.3\%$ decrease in the average rate of claims for convertible vehicle compared to the buses. The same interpretation can be extended to the other categorical covariates.


For the non-categorical variable, we need to transfer the coefficients back to the original scale to better interpret the data. The transformed values are shown below. For the vehicle value, we expect $3.59\%$ increase of the expected number of claims for every unit increase in the veh_value. Similarly, we expect $16.22\%$ increase of the expected number of claims for every unit increase in the exposure. 

```{r}
cat("Regression coefficient for veh_value after transformation to the original scale: \n",
results.car.log$summary.fixed$mean[2] * sd(dataCar$veh_value))
cat("Take the exponential:\n",
exp(results.car.log$summary.fixed$mean[2] * sd(dataCar$veh_value)))

cat("Regression coefficient for exposure after transformation to the original scale: \n",
results.car.log$summary.fixed$mean[3] * sd(dataCar$exposure))
cat("Take the exponential:\n",
exp(results.car.log$summary.fixed$mean[3] * sd(dataCar$exposure)))
```

Now, we can compute the posterior mean of the model parameter ($\mu$) in Poisson distribution across different groups.

```{r}
fittedvaluesm.B <- results.car.log$summary.fitted.values$mean
res.B <- car.data.scl %>% mutate(numclaims.mu = exp(fittedvaluesm.B))
```

We summarise the posterior average number of claims across different groups. For different vehicle bodies, panel van seems to have the largest number of claims $1.096$, while the least number of claims is given by utility. 
```{r}
res.B %>% 
  group_by(veh_body) %>%
  summarise(mean_num_claims=mean(numclaims.mu))
```

There is no huge differences of female and male in terms of the mean number of claims made. 
```{r}
res.B %>% 
  group_by(gender) %>%
  summarise(mean_num_claims=mean(numclaims.mu))
```
For the covariate are, the area F has the largest mean number of claims while the area D has the smallest number of claims.
```{r}
res.B %>% 
  group_by(area) %>%
  summarise(mean_num_claims=mean(numclaims.mu))
```

The younger people tend to make slightly more claims on average, which is consistent with the conclusion in part a that they also have a higher probability of having a claim.
```{r}
res.B %>% 
  group_by(agecat) %>%
  summarise(mean_num_claims=mean(numclaims.mu))
```

The veh_age=2 group gets the highest number of claims on average while the veh_age=4 group gets the lowest number of claims on average. Groups veh_age=1 and veh_age=3 have similar values of mean number of claims.
```{r}
res.B %>% 
  group_by(veh_age) %>%
  summarise(mean_num_claims=mean(numclaims.mu))
```



**c)[10 marks]** **Fit a zero-inflated Bayesian Poisson regression model
(<https://en.wikipedia.org/wiki/Zero-inflated_model>) on**

-   **numclaims as response,**

-   **with log link function,**

-   **using veh_value, exposure, veh_body, veh_age, gender, area, and
    agecat as covariates.**

**Center and scale the non-categorical covariates.**

**Choose your own prior distributions (do not use default priors), and
explain the rationale your prior choices, and ensure that the posterior
is not too sensitive to your prior choice [Hint: look at the induced
prior on the linear predictor and the response.]**

**Compute the posterior means of the model parameters, and discuss the
results.**

First, we look at the number of claims in the dataset. The observed proportion of zero claims is $0.932$, indicating there is a large number of zeros in the response variable (numclaims). Thus, we fit zero-inflated Poisson regression model in INLA, by setting the argument `family = "zeroinflatedpoisson1"`.
```{r}
round(prop.table(table(car.inla.data$numclaims)),5)
```


```{r}
#Priors for the regression coefficients.
prior.beta.poi <- list(mean.intercept = 0, prec.intercept =  1,
                       mean =0, prec = 25)
#Fitting the model in INLA
#"control.fixed=prior.beta" sets regression coeff. priors.
results.car.poi <- inla(formula = numclaims ~ veh_value+exposure+veh_body+
               veh_age+gender+area+agecat, family="zeroinflatedpoisson1",
             control.family=list(link="log"),
             data=car.inla.data, control.fixed=prior.beta.poi,
             control.predictor = list(compute = TRUE),
             control.compute=list(config=TRUE,cpo=TRUE, dic=TRUE))
summary(results.car.poi)
```

From the results, we can see the probability of having no claims is $0.297$, with a $95\%$ credible interval $(0.219,0.299)$. The probability of having claims is thus $0.703$, and the number of claims are given by the model process specified in the question. Notice that the intercept is $-2.101$. After taking the exponential, the we get $e^{-2.101}=0.1223$ for zero-inflated Poisson regression. We can thus compute the expectation of the number of claims of the reference group by $0.703\times 0.1223+0.297 \times 0=0.0860$. Note that this is slightly lower than the mean number of claims of the reference group given by the model in part (b) (which is $0.0862$). The interpretations of the regression coefficients are similar as part (b), except we are now conditioning on that there is a claim (or claims). Otherwise, the probability of no claim is $0.297$.

We can also compute the posterior mean number of claims of different categories. All the values are now generally lower than values we got in part (b) using standard Poisson GLM. Moreover, we observe similar facts as in part (b); e.g. for different vehicle bodies, panel van seems to have the largest mean number of claims, while the least mean number of claims is given by utility. 

```{r}
fittedvaluesm.C <- results.car.poi$summary.fitted.values$mean
res.C <- car.data.scl %>% mutate(numclaims.mu = exp(fittedvaluesm.C))
```

```{r}
res.C %>% 
  group_by(veh_body) %>%
  summarise(mean_num_claims=mean(numclaims.mu)*0.703)
```

```{r}
res.C %>% 
  group_by(gender) %>%
  summarise(mean_num_claims=mean(numclaims.mu))
```

```{r}
res.C %>% 
  group_by(area) %>%
  summarise(mean_num_claims=mean(numclaims.mu))
```

```{r}
res.C %>% 
  group_by(agecat) %>%
  summarise(mean_num_claims=mean(numclaims.mu))
```

```{r}
res.C %>% 
  group_by(veh_age) %>%
  summarise(mean_num_claims=mean(numclaims.mu))
```





**d)[10 marks] Fit a new model on numclaims in terms of the same
covariates to improve on the models in part b) or part c) by considering
interactions between covariates, as well as random effects. Describe
your new model and justify your choices.**

**Choose your own prior distributions (do not use default priors), and
explain the rationale your prior choices, and ensure that the posterior
is not too sensitive to your prior choice [Hint: look at the induced
prior on the linear predictor and the response.]**

**Compute the posterior means of the model parameters, and discuss the
results.**

We have fit a new model on numclaims in terms of the same covariates to improve on the models in part c) by considering interactions between the veh_value and exposure, as well as random effects across areas. This is because we think there might be interactions between the vehicle value and the exposure, and there might be different random effects across different area. The priors and link function used here are the same as those in part c). For the rationale, see previous explanations.

The results are shown below, where we can see the DIC decrease slightly compared with the previous model in part(c). The posterior means of model parameters can be found in the first column, and we can see them quite similar to those given by part(c). 

```{r}
#Priors for the regression coefficients.
prior.beta.poi <- list(mean.intercept = 0, prec.intercept =  1,
                       mean =0, prec = 25)

#Fitting the model in INLA
results.car.poi.rd <- inla(formula = numclaims ~ veh_value*exposure+
                             veh_body+veh_age+gender+agecat+
                             f(area, model="iid"), 
               family="zeroinflatedpoisson1",
             control.family=list(link="log"),
             data=car.inla.data, control.fixed=prior.beta.poi,
             control.predictor = list(compute = TRUE,link=1),
             control.compute=list(config=TRUE, dic=TRUE, cpo=TRUE))
summary(results.car.poi.rd)
```




**e)[10 marks] Perform posterior predictive model checks for your models
b, c, d (i.e. using replicates).**

**As test functions, use the number of rows in the dataset with
numclaims equal 0, 1, 2, 3, and 4 (5 test functions).**

**Compute the RMSE values for predicting numclaims based on all 3
models.**

**Discuss the results.**

We sampled the posterior values of the linear predictor of the three models, and then used the exponentiated values to generate posterior predictive replicates.  the posterior replicates in the Stan model code in the generated quantities block. We try
3 test functions: the median, skewness, and the mean square difference, which we define as the mean of
((y_obs[i]-y_obs[i+1])ˆ2) from i = 1 to n_obs-1 (n_obs is the number of days where we observed the
exchange rate). This mean square function is related to the volatility of the exchange rate, so it is a relevant
quantity to test our model on.

```{r}
nbsamp <- 1000
n <- nrow(car.inla.data)
yrep.b <- matrix(0,nrow=n,ncol=nbsamp)
yrep.c <- matrix(0,nrow=n,ncol=nbsamp)
yrep.d <- matrix(0,nrow=n,ncol=nbsamp)

car.b.samples=inla.posterior.sample(n=nbsamp, result=results.car.log)
car.c.samples=inla.posterior.sample(n=nbsamp, result=results.car.poi)
car.d.samples=inla.posterior.sample(n=nbsamp, result=results.car.poi.rd)

predictor.b.samples=inla.posterior.sample.eval(function(...) {Predictor},
car.b.samples)
predictor.c.samples=inla.posterior.sample.eval(function(...) {Predictor},
car.c.samples)
predictor.d.samples=inla.posterior.sample.eval(function(...) {Predictor},
car.d.samples)

p0.c <- c()
p0.d <- c()
for ( i in 1:nbsamp){
  p0.c[i] <- car.c.samples[[i]]$hyperpar
  p0.d[i] <- car.d.samples[[i]]$hyperpar[1]
}

for (row.num in 1:n){   
  yrep.b[row.num,]<- rpois(n=nbsamp,
                         lambda=exp(predictor.b.samples[row.num,]))
  
  yrep.c[row.num,]<- rzipois(n=nbsamp,
                             lambda=exp(predictor.c.samples[row.num,]),
                             pi=p0.c)  

  yrep.d[row.num,]<- rzipois(n=nbsamp,
                         lambda=exp(predictor.d.samples[row.num,]),
                         pi=p0.d)  

}
```


The root mean squared errors for the three models are computed as follows, where we can see all the three models have similar performances in terms of RMSE. Note also that the Poisson regression model in (b) gives slightly lower RMSE score.
```{r}
rmse(car.inla.data$numclaims,rowMeans(yrep.b))
rmse(car.inla.data$numclaims,rowMeans(yrep.c))
rmse(car.inla.data$numclaims,rowMeans(yrep.d))
```
```{r}
car.post.pred.test <- function(replicates,car.inla.data){
  no.clm0 <- colSums(replicates == 0)
  true.no.clm0 <- sum(car.inla.data$numclaims==0)
  
  no.clm1 <- colSums(replicates == 1)
  true.no.clm1 <- sum(car.inla.data$numclaims==1)
  
  no.clm2 <- colSums(replicates == 2)
  true.no.clm2 <- sum(car.inla.data$numclaims==2)
  
  no.clm3 <- colSums(replicates == 3)
  true.no.clm3 <- sum(car.inla.data$numclaims==3)
  
  no.clm4 <- colSums(replicates == 4)
  true.no.clm4 <- sum(car.inla.data$numclaims==4)

  par(mfrow=c(2,3))
  par(mar=c(2,2,2,2))
  
  xmin=min(c(no.clm0,true.no.clm0))-2
  xmax=max(c(no.clm0,true.no.clm0))+2
  hist(no.clm0,col="gray40",main="number of claim=0",xlim=c(xmin,xmax))
  abline(v=true.no.clm0,col="red",lwd=2)
  
  xmin=min(c(no.clm1,true.no.clm1))-2
  xmax=max(c(no.clm1,true.no.clm1))+2
  hist(no.clm1,col="gray40",main="number of claim=1",xlim=c(xmin,xmax))
  abline(v=true.no.clm1,col="red",lwd=2)
  
  xmin=min(c(true.no.clm2,no.clm2))-2
  xmax=max(c(no.clm2,true.no.clm2))+2
  hist(no.clm2,col="gray40",main="number of claim=2",xlim=c(xmin,xmax))
  abline(v=true.no.clm2, col="red", lwd=2)
  
  xmin=min(c(no.clm3,true.no.clm3))-2
  xmax=max(c(no.clm3,true.no.clm3))+2
  hist(no.clm3,col="gray40",main="number of claim=3",xlim=c(xmin,xmax))
  abline(v=true.no.clm3,col="red",lwd=2)
  
  xmin=min(c(no.clm4,true.no.clm4))-2
  xmax=max(c(no.clm4,true.no.clm4))+2
  hist(no.clm4,col="gray40",main="number of claim=4",xlim=c(xmin,xmax))
  abline(v=true.no.clm4,col="red",lwd=2)
}


```

We have performed the posterior predictive checks for all three models using the required test functions. Observe that the number of rows where the true number of claims are not zero is very small, from which we can infer the model may not fit well with the cases where number of claims is greater than 0. Although from RMSE results we may conclude the Poisson regression model in (b) would have a better overall performance, we see zero-inflated Poisson models are actually better at predicting higher number of claims, as shown below. Generally, all models seem to predict lots of zeros, which is because the dataset contains much less entries with number of claims greater than zero.
```{r}
car.post.pred.test(yrep.b,car.inla.data)
car.post.pred.test(yrep.c,car.inla.data)
car.post.pred.test(yrep.d,car.inla.data)
```



Explanation (min 300 characters in your own words, otherwise -5 marks
for insufficient explanation):

![](barcelona.jpg)

**Problem 2 - Barcelona study**

**In this problem, we will use a dataset from the CitieS-Health project
that provides insight into the impact of air pollution on humans. It is
comprised of data collected in Barcelona, Spain, and examines various
environmental variables, such as air pollution levels, and their effects
on mental health and wellbeing. In addition to environmental factors,
this dataset also captures self-reported survey data on mental health,
physical activity, diet habits, and more. From performance in a Stroop
test (a type of psychological test evaluating attention capacity and
processing speed) to information on total noise exposure at 55 dB - this
dataset contains interesting information to understand the link between
air pollution and human health.**

**We start by loading the dataset.**

```{r}
 study<-read.csv("Barcelona.csv")
 head(study)
```

**Descriptions of some of the covariates:**

| **Column name**                     | **Description**                                                                                    |
|-------------------------|-----------------------------------------------|
| **Person_ID**                       | ID of person filling out the survey (integer). Multiple rows for most persons, at different dates. |
| **date_all**                        | Date of the survey. (Date)                                                                         |
| **year**                            | Year of the survey. (Integer)                                                                      |
| **month**                           | Month of the survey. (Integer)                                                                     |
| **day**                             | Day of the survey. (Integer)                                                                       |
| **dayoftheweek**                    | Day of the week of the survey. (Integer)                                                           |
| **hour**                            | Hour of the survey. (Integer)                                                                      |
| **sadness**                         | Sadness score. (Integer)                                                                           |
| **wellbeing**                       | Self-reported survey responses regarding wellbeing. (Integer)                                      |
| **energy**                          | Self-reported survey responses regarding energy levels. (Integer)                                  |
| **stress**                          | Self-reported survey responses regarding stress levels. (Integer)                                  |
| **sleep**                           | Self-reported survey responses regarding sleep quality. (Integer)                                  |
| **hours_out**                       | Self-reported survey responses regarding time spent outdoors. (Integer)                            |
| **computer_use**                    | Self-reported survey responses regarding computer use. (Yes/No)                                    |
| **on_a\_diet**                      | Self-reported survey responses regarding diet. (Yes/No)                                            |
| **alcohol**                         | Self-reported survey responses regarding alcohol consumption. (Yes/No)                             |
| **drugs**                           | Self-reported survey responses regarding drug use. (Yes/No)                                        |
| **sick**                            | Self-reported survey responses regarding illness. (Yes/No)                                         |
| **other_factors**                   | Self-reported survey responses regarding other factors. (Yes/No)                                   |
| **stroop_test_performance**         | Performance in the Stroop test. (Float)                                                            |
| **no2bcn_24h**                      | Nitrogen dioxide (NO2) levels in Barcelona over 24 hours. (Float)                                  |
| **no2bcn_12h**                      | Nitrogen dioxide (NO2) levels in Barcelona over 12 hours. (Float)                                  |
| **no2gps_24h**                      | Nitrogen dioxide (NO2) levels in GPS locations over 24 hours. (Float)                              |
| **no2gps_12h**                      | Nitrogen dioxide (NO2) levels in GPS locations over 12 hours. (Float)                              |
| **no2bcn_12h_x30**                  | Nitrogen dioxide (NO2) levels in Barcelona over 12 hours multiplied by 30. (Float)                 |
| **no2bcn_24h_x30**                  | Nitrogen dioxide (NO2) levels in Barcelona over 24 hours multiplied by 30. (Float)                 |
| **no2gps_12h_x30**                  | Nitrogen dioxide (NO2) levels in GPS locations over 12 hours multiplied by 30. (Float)             |
| **no2gps_24h_x30**                  | Nitrogen dioxide (NO2) levels in GPS locations over 24 hours multiplied by 30. (Float)             |
| **min_gps**                         | Minimum GPS location. (Float)                                                                      |
| **district**                        | District of Barcelona where the survey was conducted. (String)                                     |
| **education**                       | Educational level of the participant. (String)                                                     |
| **maxwindspeed_12h**                | Maximum wind speed over 12 hours. (Float)                                                          |
| **access_greenbluespaces_300mbuff** | Access to green and blue spaces within a 300m buffer. (Yes/No)                                     |
| **microgram3**                      | Micrograms per cubic meter of pollutants. (Float)                                                  |
| **age_yrs**                         | Age of the participant in years. (Integer)                                                         |
| **yearbirth**                       | Year of birth of the participant. (Integer)                                                        |
| **smoke**                           | Self-reported survey responses regarding smoking status. (Yes/No)                                  |
| **gender**                          | Gender of the participant. (Woman/Man)                                                             |
| **hour_gps**                        | Hour of the GPS location. (Integer)                                                                |
| **pm25bcn**                         | Particulate matter (PM2.5) levels in Barcelona. (Float)                                            |
| **BCmicrog**                        | Black carbon (BC) levels in micrograms. (Float)                                                    |
| **sec_noise55_day**                 | Seconds of noise over 55 minutes in a day. (Integer)                                               |
| **sec_noise65_day**                 | Seconds of noise over 65 minutes in a day. (Integer)                                               |
| **tmean_24h**                       | Mean temperature over 24 hours. (Float)                                                            |
| **tmean_12h**                       | Mean temperature over 12 hours. (Float)                                                            |
| **humi_24h**                        | Humidity over 24 hours. (Float)                                                                    |
| **humi_12h**                        | Humidity over 12 hours. (Float)                                                                    |
| **pressure_24h**                    | Pressure over 24 hours. (Float)                                                                    |
| **pressure_12h**                    | Pressure over 12 hours. (Float)                                                                    |
| **precip_24h**                      | Precipitation over 24 hours. (Float)                                                               |
| **precip_12h**                      | Precipitation over 12 hours. (Float)                                                               |
| **precip_12h_binary**               | Binary value for precipitation over 12 hours. (Integer)                                            |
| **precip_24h_binary**               | Binary value for precipitation over 24 hours. (Integer)                                            |
| **maxwindspeed_24h**                | Maximum wind speed over 24 hours. (Float)                                                          |

**You can use either JAGS, Stan, or INLA for this question.**

**a)[10 marks] Fit a Bayesian linear regression model**

-   **on the logarithm of stroop_test_performance as response,**

-   **using the following covariates: gender, on_a\_diet, alcohol,
    drugs, sick, other_factors, educational, smoke, no2gps_24h,
    maxwindspeed_24h, precip_24h, sec_noise55_day,
    access_greenbluespaces_300mbuff, age_yrs, tmean_24h (you can use
    categorical covariates by converting integers to factors if
    appropriate).**

**Center and scale the non-categorical covariates.**

**Choose your own prior distributions (do not use default priors), and
explain the rationale your prior choices, and ensure that the posterior
is not too sensitive to your prior choice [Hint: look at the induced
prior on the response.]**

**Compute the posterior means of the model parameters, and interpret
their meaning.**

We first create the dataset for model fitting. Since there are missing values in some of the covariates, we remove them. The non-categorical variables are scaled. For stroop_test_performance, we first take the log and then scale them.
```{r}
study.data.scl <- study %>% 
  select(Person_ID,gender, on_a_diet, alcohol, drugs, sick, other_factors, 
         education, smoke,no2gps_24h, maxwindspeed_24h,precip_24h,
         sec_noise55_day,access_greenbluespaces_300mbuff,age_yrs,
         tmean_24h, stroop_test_performance, sadness) %>%
  mutate(no2gps_24h=scale(no2gps_24h),maxwindspeed_24h=scale(maxwindspeed_24h),
         precip_24h=scale(precip_24h),sec_noise55_day=scale(sec_noise55_day),
         age_yrs=scale(age_yrs),tmean_24h=scale(tmean_24h),
         log.stroop_test_performance=log(stroop_test_performance)) %>%
  mutate_all(~ifelse(. %in% c("N/A", "null", ""), NA, .)) %>% 
  na.omit()

```

In terms of prior choices, we would like to ensure that the linear predictor has a large probability of being in the interval $[3,5]$. A density plot of the scaled and transformed true responses are shown below.
```{r}
plot(density(study.data.scl$log.stroop_test_performance))
```

We set the intercept to have mean $4$ and standard deviation $1$. We use the same variance for each of the regression coefficients. First, we compute the sum of the covariate squares for each row in the dataset, and then use the $0.05$ quantile of this. Then we choose the variance of the regression coefficients as $\sigma_{\beta}^2=1/(0.05\text{quantile of sum of covariates squares})$.

```{r}
mm <- model.matrix(sadness ~ gender+on_a_diet+alcohol+
                  drugs+sick+other_factors+education+smoke+
                  no2gps_24h+maxwindspeed_24h+precip_24h+sec_noise55_day+
                  access_greenbluespaces_300mbuff+age_yrs+tmean_24h,
                data=study.data.scl)
var.beta <- 1/quantile(rowSums(mm^2),0.05)
cat("Variance of regression coefficients:",var.beta, "\n")
```

Now, we look at the induced prior on the linear predictor (as well as the exponential of linear predictor) in a spacial case (where all the covariates are $1$ for non-categorical ones). Our choice of priors looks acceptable. 
```{r}
x <- matrix(nrow=1000,ncol=15)
x[,1] <- rnorm(1000,mean=4,sd=sqrt(1))
for (i in 2:15){
  x[,i] <- rnorm(1000,sd=sqrt(0.2))
}

par(mfrow=c(1,2))
plot(density((rowSums(x))),main="log scale")

plot(density(exp(rowSums(x))),xlim=c(0,100),
     main="original scale")

```

We fit the model as required in INLA. The summary including the posterior mean of model parameters are shown below. 
```{r}
#Priors for the regression coefficients.
prior.beta.a <- list(mean.intercept = 4, prec.intercept =  1,
                       mean =0, prec = 5)

prec.prior <- list(prec=list(prior = "loggamma", param = c(0.1, 0.1)))
#Fitting the model in INLA
#"control.fixed=prior.beta" sets regression coeff. priors.
results.study.a <- inla(formula = log.stroop_test_performance ~ gender+on_a_diet+
                          alcohol+drugs+sick+other_factors+education+smoke+
                          no2gps_24h+maxwindspeed_24h+precip_24h+sec_noise55_day+
                          access_greenbluespaces_300mbuff+age_yrs+tmean_24h,
                        family="gaussian",
                        control.family=list(hyper=prec.prior),
                        data=study.data.scl, control.fixed=prior.beta.a,
                        control.predictor = list(compute = TRUE),
                        control.compute=list(config=TRUE,cpo=TRUE, dic=TRUE))
summary(results.study.a)
```

The model fit indicates that the intercept has mean $3.705$. We can take the exponential to get the estimated mean value of the stroop test performance which is $40.65$.
```{r}
exp(3.705)
```

Therefore, the typical number of stroop test performance is close to $40.65$. It is worth noting that genderOtra and genderWoman have negative mean values, indicating that men tend to have higher test performance. The educationUniversity coefficient has mean $0.14$, indicating that there is a university-level education advantage of around $e^{0.14}=1.15$ scores. The coefficient for age_yrs is $-0.124$. We can transform to the original scale as follows. Hence, we expect $0.219$ unit increase of the expected test performance for one year increase in age. The same way of interpretation can be applied to all non-categorical variables.
```{r}
exp(-0.124*sd(study$age_yrs))
```


```{r}
glm(log.stroop_test_performance ~ gender+on_a_diet+
                          alcohol+drugs+sick+other_factors+education+smoke+
                          no2gps_24h+maxwindspeed_24h+precip_24h+sec_noise55_day+
                          access_greenbluespaces_300mbuff+age_yrs+tmean_24h,
    family=gaussian,data=study.data.scl)

```


**b)[10 marks] Fit a Bayesian Poisson GLM**

-   **for sadness as response,**

-   **log link function,**

-   **using the following covariates: gender, on_a\_diet, alcohol,
    drugs, sick, other_factors, educational, smoke, no2gps_24h,
    maxwindspeed_24h, precip_24h, sec_noise55_day,
    access_greenbluespaces_300mbuff, age_yrs, tmean_24h (you can use
    categorical covariates by converting integers to factors if
    appropriate).**

**Center and scale the non-categorical covariates.**

**Choose your own prior distributions (do not use default priors), and
explain the rationale your prior choices, and ensure that the posterior
is not too sensitive to your prior choice [Hint: look at the induced
prior on the response.]**

**Compute the posterior means of the model parameters, and interpret
their meaning.**

First we look at the density of the response variable, sadness, both in log scale and original scale.
```{r}
par(mfrow=c(1,2))
plot(density(log(study.data.scl$sadness)), main="log scale")
plot(density((study.data.scl$sadness)),main="original scale")
```

The typical values of (scaled) sadness scores are in the interval $[0,15]$, so we would like to the induced prior on the sadness scores to have a large probability density in that interval. We tried to set the prior for intercept as $\mathcal{N}(0,10)$ and set the priors for each regression coefficients to be identically $\mathcal{N}(0,0.1)$. Then, we can plot the induced prior for the sadness scores in different covariate settings. After trying different values of covariates, we found our induced priors look alright. 
```{r}
x <- matrix(nrow=1000,ncol=15)
x[,1] <- rnorm(1000,mean=0,sd=sqrt(1))

# For categorical covaraites, we randomly pick one category 
# in each group. 
# So, in total we have 15 coefficients (excluding the intercept)
for (i in 2:15){
  x[,i] <- rnorm(1000,mean=0,sd=sqrt(0.2))
}
par(mfrow=c(1,3))
plot(density(exp(rowSums(x))),xlim=c(0,50))

# set some values for covariates
b <- rep(1,15)
b[9] <- as.numeric(quantile(study.data.scl$no2gps_24h,0.95))
b[10] <- as.numeric(quantile(study.data.scl$maxwindspeed_24h,0.95))
b[11] <- as.numeric(quantile(study.data.scl$precip_24h,0.95))
b[12] <- as.numeric(quantile(study.data.scl$sec_noise55_day,0.95))
b[14] <- as.numeric(quantile(study.data.scl$age_yrs,0.95))
b[15] <- as.numeric(quantile(study.data.scl$tmean_24h,0.95))
x <- x%*% b

plot(density(exp(rowSums(x))),xlim=c(0,50))


b <- rep(1,15)
b[9] <- as.numeric(quantile(study.data.scl$no2gps_24h,0.05))
b[10] <- as.numeric(quantile(study.data.scl$maxwindspeed_24h,0.05))
b[11] <- as.numeric(quantile(study.data.scl$precip_24h,0.05))
b[12] <- as.numeric(quantile(study.data.scl$sec_noise55_day,0.05))
b[14] <- as.numeric(quantile(study.data.scl$age_yrs,0.05))
b[15] <- as.numeric(quantile(study.data.scl$tmean_24h,0.05))
x <- x%*% b

plot(density(exp(rowSums(x))),xlim=c(0,50))
```
```{r}
glm(sadness ~ gender+on_a_diet+
                          alcohol+drugs+sick+other_factors+education+smoke+
                          no2gps_24h+maxwindspeed_24h+precip_24h+sec_noise55_day+
                          access_greenbluespaces_300mbuff+age_yrs+tmean_24h,
    family=poisson(link="log"),data=study.data.scl)
```

We implement the model in INLA with the priors for the intercept and regression coefficients specified above. We also tried different Normal priors with mean zero slightly different precision hyperparameters, and the results are similar, showing our choice of prior is not sensitive. The posterior means of the model parameters are shown in the summary below.
```{r}
#Priors for the regression coefficients.

# Tryied differet priors and the results look similar,
# indicating that our choice of prior is not sensitive 
#prior.beta.b <- list(mean.intercept = 0, prec.intercept =  1,
#                       mean =0, prec = 1)
prior.beta.b <- list(mean.intercept = 0, prec.intercept =  1,
                       mean =0, prec = 5)


#Fitting the model in INLA
#"control.fixed=prior.beta" sets regression coeff. priors.
results.study.b <- inla(formula = sadness ~ gender+on_a_diet+
                          alcohol+drugs+sick+other_factors+education+smoke+
                          no2gps_24h+maxwindspeed_24h+precip_24h+sec_noise55_day+
                          access_greenbluespaces_300mbuff+age_yrs+tmean_24h,
                        family="poisson",
                        control.family=list(link="log"),
                        data=study.data.scl, control.fixed=prior.beta.b,
                        control.predictor = list(compute = TRUE,link=1),
                        control.compute=list(config=TRUE,cpo=TRUE, dic=TRUE))
summary(results.study.b)
```

The intercept has mean $1.758$. Taking the exponential we get the estimated mean value of the sadness level, which is $5.80$. Note that the genderOtra has value $-0.430$, indicating that the non-binary people tend to be more likely to be sad. More specifically, the statistic shows there is a $1-e^{-0.43}=35\%$ decrease in the average rate of sadness level for convertible vehicle compared to the men, assuming other factors remain the same. 

**c)[10 marks] Incorporate Person_ID as a random effects into the models
a.) and b.).**

**Choose your own prior distributions for this random effect (do not use
default priors).**

**Compare the posterior means of the parameter values with a) and b).**

**Discuss the changes that happened due to using random effects.**

In this part, we incorporate Person_ID as a random effects into the two models in (a) and (b) respectively. We choose to adopt the penalized complexity priors for both the models. To calibrate the scaling of the random effects prior, we set $U$ and $p$ so that $P(\sigma u>U) =p$. We set $p=0.01$ and fix $U=10\sigma_y$, as we believe the probability that the standard deviation of the random effect is $10$ times that of the response is quite small. 

```{r}
# sd of the log stroop test performance
sdres.a <- sd(study.data.scl$log.stroop_test_performance)
# sd of the sadness scores
sdres.b <- sd(study.data.scl$sadness)
```

The posterior summary of the model parameters are shown as follows. 
```{r}
# Priors for the random effects
pcprior.c1 <- list(prec = list(prior="pc.prec", param = c(10*sdres.a,0.01)))
#Priors for the regression coefficients.
prior.beta.a <- list(mean.intercept = 4, prec.intercept =  1,
                       mean =0, prec = 5)

prec.prior <- list(prec=list(prior = "loggamma", param = c(0.1, 0.1)))
#Fitting the model in INLA
#"control.fixed=prior.beta" sets regression coeff. priors.
results.study.c1 <- inla(formula = log.stroop_test_performance ~ gender+on_a_diet+
                          alcohol+drugs+sick+other_factors+education+smoke+
                          no2gps_24h+maxwindspeed_24h+precip_24h+sec_noise55_day+
                          access_greenbluespaces_300mbuff+age_yrs+tmean_24h+
                           f(Person_ID, model="iid",hyper=pcprior.c1),
                        family="gaussian",
                        control.family=list(hyper=prec.prior),
                        data=study.data.scl, control.fixed=prior.beta.a,
                        control.predictor = list(compute = TRUE),
                        control.compute=list(config=TRUE,cpo=TRUE, dic=TRUE))
summary(results.study.c1)
```

Compared with the model in (a), this model with random effects on person ID has regression coefficients generally smaller in absolute value. More specifically, $11$ out of $18$ coefficients are smaller in absolute value. Moreover, the posterior mean of the precision for the Gaussian observations becomes larger, indicating that some of the variance is explained by the random effects introduced. The mean of the precision for Person_ID is $38.75$, which is a relatively clear sign of overdispersion.
```{r}
cat("Number of regression coefficients that are larger in absolute value: ",
sum(abs(results.study.a$summary.fixed$mean)>abs(results.study.c1$summary.fixed$mean)))
cat("\nThe following covariates have coefficients that become larger than the model without random effects: \n",results.study.a$names.fixed[abs(results.study.a$summary.fixed$mean)>abs(results.study.c1$summary.fixed$mean)])
cat("\nThe following covariates have coefficients that become smaller than the model without random effects: \n",results.study.a$names.fixed[abs(results.study.a$summary.fixed$mean)<abs(results.study.c1$summary.fixed$mean)])
```

The summary for the second model after adding the random effects on ID is shown below. The precision for ID is $12.04$, showing a clear sign of overdispersion. As expected, the DIC given by this model is smaller.
```{r}
prior.beta.c2 <- list(mean.intercept = 0, prec.intercept =  1,
                       mean =0, prec = 5)
pcprior.c2 <- list(prec = list(prior="pc.prec", param = c(10*sdres.b,0.01)))
#Fitting the model in INLA
#"control.fixed=prior.beta" sets regression coeff. priors.
results.study.c2 <- inla(formula = sadness ~ gender+on_a_diet+
                          alcohol+drugs+sick+other_factors+education+smoke+
                          no2gps_24h+maxwindspeed_24h+precip_24h+sec_noise55_day+
                          access_greenbluespaces_300mbuff+age_yrs+tmean_24h+
                          f(Person_ID,model="iid",hyper = pcprior.c2 ),
                        family="poisson",
                        control.family=list(link="log"),
                        data=study.data.scl, control.fixed=prior.beta.c2,
                        control.predictor = list(compute = TRUE,link=1),
                        control.compute=list(config=TRUE,cpo=TRUE, dic=TRUE))
summary(results.study.c2)
```

Similarly, we observe many regression coefficients become smaller in absolute value. 
```{r}
cat("number of regression coefficients that are larger in absolute value: ",
sum(abs(results.study.b$summary.fixed$mean)>abs(results.study.c2$summary.fixed$mean)))
cat("\nThe following covariates have coefficients that become larger than the model without random effects: \n",results.study.b$names.fixed[abs(results.study.b$summary.fixed$mean)>abs(results.study.c2$summary.fixed$mean)])
cat("\nThe following covariates have coefficients that become smaller than the model without random effects: \n",results.study.b$names.fixed[abs(results.study.b$summary.fixed$mean)<abs(results.study.c2$summary.fixed$mean)])
```


**d)[10 marks] Do posterior predictive checks (i.e. using replicates)
for the sadness score for your models with or without random effects.
Explain the choice of test functions that you used.**

**Compute the posterior means of the response variable using the
original covariates, and use this to compute the RMSE values for both
models (i.e. with, or without random effects).**

**Discuss the results.**

We first draw $1000$ samples for each linear predictor of the model, and use each linear predictor as the rate parameter in Poisson distribution to draw the response variable "sadness". Therefore, for each model we get $1766\times 1000$ replication matrix.

```{r}
nbsamp <- 1000
n2 <- nrow(study.data.scl)
yrep.sad.b <- matrix(0,nrow=n2,ncol=nbsamp)
yrep.sad.c <- matrix(0,nrow=n2,ncol=nbsamp)

sad.b.samples=inla.posterior.sample(n=nbsamp, result=results.study.b)
sad.c.samples=inla.posterior.sample(n=nbsamp, result=results.study.c2)

predictor.sad.b.samples=inla.posterior.sample.eval(function(...) {Predictor},
sad.b.samples)
predictor.sad.c.samples=inla.posterior.sample.eval(function(...) {Predictor},
sad.c.samples)

for (row.num in 1:n2){   
  yrep.sad.b[row.num,]<- rpois(n=nbsamp,
                         lambda=exp(predictor.sad.b.samples[row.num,]))
  yrep.sad.c[row.num,]<- rpois(n=nbsamp,
                         lambda=exp(predictor.sad.c.samples[row.num,]))

}
```

To perform the posterior predictive check, we use the following three test functions:

1. number of people having different sadness scores.
2. average number of sadness scores among groups with different true sadness scores. 

```{r}
sad.post.pred.test1 <- function(replicates,study.data.scl){
  par(mfrow=c(4,4))
  par(mar=c(1.7,1.7,1.7,1.7))
  for (i in 1:14){
    no.sad <- colSums(replicates==i)
    true.no.sad <- sum(study.data.scl$sadness==i)
    xmin=min(c(no.sad,true.no.sad))-2
    xmax=max(c(no.sad,true.no.sad))+2
    hist(no.sad,col="gray40",main=c("sadness",i),xlim=c(xmin,xmax))
    abline(v=true.no.sad,col="red",lwd=2)
  }
}

sad.post.pred.test2 <- function(replicates,study.data.scl){
  par(mfrow=c(4,4))
  par(mar=c(1.7,1.7,1.7,1.7))
  for (i in 1:14){
    ind.sad <- which(study.data.scl$sadness==i)
    sad <- colMeans(replicates[ind.sad,])
    xmin=min(c(sad,i))-2
    xmax=max(c(sad,i))+2
    hist(sad,col="gray40",main=c("sadness=",i),xlim=c(xmin,xmax))
    abline(v=i,col="red",lwd=2)
  }
}
```

Below are the results for the test function 1 for the model in (b) on the top block and the model in (c) in the bottom block respectively. We can see that both models have a poor fit for the number of people having sadness score equal to $1,2,4,5,6,7,8,11,12,13,14$, with the value of this function on the dataset being far away from the typical realisations according to the replicates. However, the model in (c) has a smaller discrepancy compared to the model in (b). This indicates that the model without random effects on ID explain the data even worse than the model considering the random effects. For the sadness score equal to $3,9,10$, the two model have satisfactory fit, and note that the results for the model in (c) still looks slightly better than those for the model in (b).

```{r}
sad.post.pred.test1(yrep.sad.b,study.data.scl)
sad.post.pred.test1(yrep.sad.c,study.data.scl)
```

Here are the results for model in (b) on the top block and for the model in (c) on the bottom block using the test function 3. The test results are satisfactory for the group where the sadness score is $6$ and $7$. While the two models show a poor fit for this test function, we notice that the model in (c) is slightly better than the model in (b).

```{r}
sad.post.pred.test2(yrep.sad.b,study.data.scl)

sad.post.pred.test2(yrep.sad.c,study.data.scl)

```


The root mean squared errors for the two models are computed below. We see a significant improvement of the model in terms of the RMSE if we add random effects of ID to the model.
```{r}
cat("RMSE for the model in (b):",
rmse(study.data.scl$sadness,rowMeans(yrep.sad.b)))
cat("\nRMSE for the model in (c):",
rmse(study.data.scl$sadness,rowMeans(yrep.sad.c)))
```



**e)[10 marks]**

**Plot the posterior predictive distributions for
stroop_test_performance and sadness for the random effect models in part
c) for the following new person in the dataset:**

**Person_ID=286, gender="Woman", on_a\_diet="Yes", alcohol="No",
drugs="No", sick="No", other_factors="No", education="University",
smoke="Yes", no2gps_24h=80, maxwindspeed_24h=10, precip_24h=50,
sec_noise55_day=10000, access_greenbluespaces_300mbuff="Yes",
age_yrs=40, tmean_24h=25**

**In the case of stroop_test_performance, plot the estimated density,
while for sadness, plot a histogram.**

**Compute the posterior predictive mean, and standard deviation.**

**Discuss the results.**

We begin with adding these covariates as a new row to the dataset, and then set the response variable as NA.
```{r}
study.data.new  <- 
  data.frame(Person_ID=286, 
             gender="Woman", on_a_diet="Yes", alcohol="No",
             drugs="No", sick="No",other_factors="No",
             education="University",smoke="Yes", 
             no2gps_24h=(80-mean(study$no2gps_24h))/sd(study$no2gps_24h),
             maxwindspeed_24h=(10-mean(study$maxwindspeed_24h))/sd(study$maxwindspeed_24h),
             precip_24h=(50-mean(study$precip_24h))/sd(study$precip_24h),
             sec_noise55_day=(10000-mean(study$sec_noise55_day))/sd(study$sec_noise55_day),
             access_greenbluespaces_300mbuff="Yes",
             age_yrs=(40-mean(study$age_yrs))/sd(study$age_yrs), 
             tmean_24h=(25-mean(study$tmean_24h))/sd(study$tmean_24h),
             sadness=NA,stroop_test_performance=NA,
             log.stroop_test_performance=NA) %>%
  rbind(study.data.scl)

```

The second step is to fit the model using the new dataset. 
```{r}
results.study.a <- inla(formula = log.stroop_test_performance ~ gender+on_a_diet+
                          alcohol+drugs+sick+other_factors+education+smoke+
                          no2gps_24h+maxwindspeed_24h+precip_24h+sec_noise55_day+
                          access_greenbluespaces_300mbuff+age_yrs+tmean_24h,
                        family="gaussian",
                        control.family=list(hyper=prec.prior),
                        data=study.data.new, control.fixed=prior.beta.a,
                        control.predictor = list(compute = TRUE),
                        control.compute=list(config=TRUE,cpo=TRUE, dic=TRUE))

results.study.c2 <- inla(formula = sadness ~ gender+on_a_diet+
                          alcohol+drugs+sick+other_factors+education+smoke+
                          no2gps_24h+maxwindspeed_24h+precip_24h+sec_noise55_day+
                          access_greenbluespaces_300mbuff+age_yrs+tmean_24h+
                          f(Person_ID,model="iid" ),
                        family="poisson",
                        control.family=list(link="log"),
                        data=study.data.new, control.fixed=prior.beta.c2,
                        control.predictor = list(compute = TRUE,link=1),
                        control.compute=list(config=TRUE,cpo=TRUE, dic=TRUE))
```

After this, we obtain posterior samples for the Predictor variable for this new data point  (located in the first row). The Predictor variables in the sample correspond to the linear predictor in the LGM model. To get the samples of the response, we need to draw from Gaussian/Poisson distribution using the linear predictor samples as mean and sd (/rate for Poisson) parameters. 

In the following, we generate $10000$ posterior predictive samples for the model in (a). For the new data point, the posterior  predictive mean is around $42.9$, the posterior predictive standard deviation is around $9.6$. 
```{r}
nsamp <- 10000
log.stroop.samples <- inla.posterior.sample(n=nsamp,
                                        result=results.study.a,
                                        selection = list(Predictor=1))
   #selection = list(Predictor=1) means that we only want
   #the samples for the linear predictor eta_i of the new datapoint
ls.predictor.samples <- unlist(lapply(log.stroop.samples, function(x)(x$latent[1])))

ls.sigma.samples <- 1/sqrt(unlist(lapply(log.stroop.samples, 
                                      function(x)(x$hyperpar[1])))) 
ls.post.pred.samples <- ls.predictor.samples +
  rnorm(n=nsamp,mean=0,sd=ls.sigma.samples)

plot(density(exp(ls.post.pred.samples)),
     xlab="x",ylab="Density", 
     main="Posterior predictive for stroop test of the new datapoint")
cat("The posterior predictive mean for stroop test performance is ",
    mean(exp(ls.post.pred.samples)),"\n",
    "The posterior predictive standard deviation for stroop test performance is ",
    sd(exp(ls.post.pred.samples)))
```

Similarly, we get $10000$ posterior predictive samples for the model in (c). For the new data point, the posterior  predictive mean is around $1.46$ and the posterior predictive standard deviation is around $1.25$. The person has the sadness score $1.46$. We can use MAP as the estimate and thus conclude that the person will have a sadness score $2$.

```{r}
nsamp <- 10000
sad.c2.samples <- inla.posterior.sample(n=nsamp,
                                        result=results.study.c2,
                                        selection = list(Predictor=1))
   #selection = list(Predictor=1) means that we only want
   #the samples for the linear predictor eta_i of the new datapoint
sad.predictor.samples <- unlist(lapply(sad.c2.samples, function(x)(x$latent[1])))
sad.post.pred.samples <- rpois(n=nsamp, lambda=sad.predictor.samples)

barplot(table(sad.post.pred.samples)/length(sad.post.pred.samples),
        xlab="x",ylab="Density", 
     main="Posterior predictive for sadness scores of the new datapoint")

cat("The posterior predictive mean for stroop test performance is ",
    mean(sad.post.pred.samples),"\n",
    "The posterior predictive standard deviation for stroop test performance is ",
    sd(sad.post.pred.samples))
```