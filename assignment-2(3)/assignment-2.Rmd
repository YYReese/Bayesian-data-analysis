---
editor_options:
  markdown:
    wrap: 72
output: pdf_document
---

**University of Edinburgh**

**School of Mathematics**

**Bayesian Data Analysis, 2022/2023, Semester 2**

**Assignment 2**

**IMPORTANT INFORMATION ABOUT THE ASSIGNMENT**



```{r}
rm(list = ls(all = TRUE))
#Do not delete this!
#It clears all variables to ensure reproducibility
```

![](car_insurance.jpg)

**Problem 1**

**In this problem, we study a dataset about car insurance.** **This data
set is based on one-year vehicle insurance policies taken out in 2004 or
2005. In total, there are 67856 policies, of which 4624 have claims.**

```{r}
require(insuranceData)
require(dplyr)
require(knitr)
require(dummies)
require(Metrics)
require(distributions3)
require(INLA)
data(dataCar)

#The first 6 rows of the dataframe
print.data.frame(dataCar[1:6,])

```

**Description of the columns.**

**veh_value: vehicle value in \$10000s**

**exposure: maximum portion of the vehicle value the insurer may need to
pay out in case of an incident**

**claimcst0: claim amount (0 if no claim)**

**clm: whether there was a claim during the 1 year duration**

**numclaims: number of claims during the 1 year duration**

**veh_body types: BUS = bus CONVT = convertible COUPE = coupe HBACK =
hatchback HDTOP = hardtop MCARA = motorized caravan MIBUS = minibus
PANVN = panel van RDSTR = roadster SEDAN = sedan STNWG = station wagon
TRUCK = truck UTE = utility**

**gender: F- female, M - male\
\
area: a factor with levels A,B,C,D,E, F**

**agecat: age category, 1 (youngest), 2, 3, 4, 5, 6**

**You can use either JAGS, Stan, or INLA for this question.**

**a)[10 marks] Fit a Bayesian logistic regression model on the dataset
dataCar with**

-   **clm as response,**

-   **a link function of your choice,**

-   **using veh_value, exposure, veh_body, veh_age, gender, area, and
    agecat as covariates (you can use categorical covariates by
    converting integers to factors if appropriate).**

**Center and scale the non-categorical covariates.**

**Choose your own prior distributions (do not use default priors), and
explain the rationale your prior choices, and ensure that the posterior
is not too sensitive to your prior choice [Hint: look at the induced
prior on the linear predictor and on the response.]**

**Compute the posterior means of the model parameters, and discuss the
results.**

```{r}
# Define the functions 
odds <- function(x) x/(1-x) # odds ratio function
logit <- function(x) log(x/(1-x))
ilogit <- function(x){ 1/(1+exp(-x))} #Inverse logistic function
```

In this part, we standardise two non-categorical covariates, veh_value and exposure, and convert veh_age to categorical variable.
```{r}
# Data preprocessing 
car.data.scl <- dataCar %>% 
  select(veh_value, exposure,veh_body, 
         veh_age, gender, area, agecat, clm,numclaims) %>% 
  mutate(veh_value=scale(veh_value), 
         exposure=scale(exposure))

####Car data with categorical covariates - INLA ####
car.inla.data <- car.data.scl %>% 
  select(veh_value, exposure,veh_body, 
         veh_age, gender, area, agecat, clm, numclaims) %>%
  mutate(veh_body=as.factor(veh_body),area=as.factor(area),
         agecat=as.factor(agecat),veh_age=as.factor(veh_age))
```

### Data exploration
In the following, we compute the odds ratio of different groups of the data which might be useful for prior choices. 
```{r}
odds.body <- c()
for (i in 1:13){
  body <- unique(car.inla.data$veh_body)[i]
  df.body <- car.inla.data %>% filter(veh_body %in% c(body)) 
  odds.body[i] <- odds(sum(df.body$clm == 1)/nrow(df.body))
}
body.ratio <- max(odds.body)/min(odds.body)
body.ratio 

odds.gender <- c()
df.m <- car.inla.data %>% filter(gender %in% c("M"))
df.f <- car.inla.data %>% filter(gender %in% c("F"))
odds.gender[1] <- odds(sum(df.m$clm == 1)/nrow(df.m))
odds.gender[2] <- odds(sum(df.f$clm == 1)/nrow(df.f))
gender.ratio <- max(odds.gender)/min(odds.gender)
gender.ratio

odds.area <- c()
for (i in 1:6){
  a <- unique(car.inla.data$area)[i]
  df.area <- car.inla.data %>% filter(area %in% c(a)) 
  odds.area[i] <- odds(sum(df.area$clm == 1)/nrow(df.area))
}
area.ratio <- max(odds.area)/min(odds.area)
area.ratio

odds.agecat <- c()
for (i in 1:6){
  age <- unique(car.inla.data$agecat)[i]
  df.agecat <- car.inla.data %>% filter(agecat %in% c(age)) 
  odds.agecat[i] <- odds(sum(df.agecat$clm == 1)/nrow(df.agecat))
}
agecat.ratio <- max(odds.agecat)/min(odds.agecat)
agecat.ratio


odds.v_age <- c()
for (i in 1:4){
  v_age <- unique(car.inla.data$veh_age)[i]
  df.v_age <- car.inla.data %>% filter(veh_age %in% c(v_age)) 
  odds.v_age[i] <- odds(sum(df.v_age$clm == 1)/nrow(df.v_age))
}
v_age.ratio <- max(odds.v_age)/min(odds.v_age)
v_age.ratio

```
From the results, we can see that for veh_body, the odds ratio of one group can be at most $6$ times lager than that of another group. For gender, the odds ratio of female is $0.017$ times lager than that of male. For area, the odds ratio of one group is at most $1.314$ times larger than that of another group. For agecat, the odds ratio of one group is at most $1.601$ times larger than that of another group. For veh_age, the odds ratio of one group is at most $1.239$ times larger than that of another group. Using the information, we set the priors for the regression coefficients of linear predictors as follows.
\begin{itemize}
  \item For veh_body: we want $e^{\beta_3}$ to have large mass at around $6$, so it is reasonable to set prior $\mathcal{N}(0,\sigma_3^2)$ where $\sigma_3=\ln 6$. 
  \item For gender: we want $e^{\beta_5}$ to have large mass at around $1.02$, so we may like to put prior $\mathcal{N}(0,\sigma_5^2)$ where $\sigma_5=\ln 1.02$. However, this looks a bit informative, so we let $\sigma_5=1$ instead. It cause no problem to change the prior to make it vaguer. 
  \item For area: we want $e^{\beta_6}$ to have large mass at around $1.31$, so it is reasonable to set prior $\mathcal{N}(0,\sigma_6^2)$ where $\sigma_6=\ln 1.31$. 
  \item For agecat: we want $e^{\beta_7}$ to have large mass at around $1.6$, so it is reasonable to set prior $\mathcal{N}(0,\sigma_7^2)$ where $\sigma_7=\ln 1.6$. 
  \item For veh_age: we want $e^{\beta_4}$ to have large mass at around $1.24$, so it is reasonable to set prior $\mathcal{N}(0,\sigma_6^2)$ where $\sigma_6=\ln 1.24$. 
  \item For veh_value: we want $e^{\beta_1 x_1}$ to have large mass at around $5$. Having checked the density of scaled value of veh_value variable, we think it takes values mostly between $0$ to $5$. Therefore, it is sensible to set prior $\mathcal{N}(0,\sigma_1^2)$ where $\sigma_1=\ln 5/5$. 
  \item For exposure: we want $e^{\beta_2 x_2}$ to have large mass at around $1.5$. Having check the density of scaled value of exposure, we found it takes values mostly within $-1$ to $1$. Therefore, it is sensible to set prior $\mathcal{N}(0,\sigma_2^2)$ where $\sigma_2=\ln 1.5/1=\ln 1.5$. 
  For the intercept $beta_0$: we want to ensure that the inverse logit of $30$ times $|\beta_0|$ can be in the interval $(-0.05,0.95)$. (Here $30$ is roughly the exponential of all the covariates' effects, i.e. $50\approx e^{\beta_1 x_1 \beta_2 x_2 ...\beta_7}$.) Hence, we come up with the prior $\mathcal{N}(0, \sigma_0)$ for $\beta_0$ where $\sigma_0=log(6)$.
\end{itemize}

Now let us see the induced prior on the probability parameter in Bernoulli distribution $p_i$. We simulate $1000$ points for each beta. Assume we have each all non-categorical covariates equal to 1 (after scaling) and randomly choose a group for all the categorical variables. Then we can compute the responses. After that, the inverse logit of the responses can be computed so that we can plot the density of the probability parameter in Bernoulli distribution, $p_i$. From the following plot, we can see the induced prior on $\mu$ looks okay. We have also tried different values of $x_1,x_2,...$ and the results showed no problem, so we continue with our choices of priors.
```{r}
beta0 <- rnorm(1000,sd=1.79)
beta1 <- rnorm(1000,sd=log(5)/5)
beta2 <- rnorm(1000,sd=log(1.5))
beta3 <- rnorm(1000,sd=log(6))
beta4 <- rnorm(1000,sd=log(1.24))
beta5 <- rnorm(1000,sd=log(1.02))
beta6 <- rnorm(1000,sd=log(1.31))
beta7 <- rnorm(1000,sd=log(1.6))
x <- ilogit( beta0+beta1+beta2+beta3+beta4+beta5+beta6+beta7)
plot(density(x),main="density of the induced prior on the parameter p in Bernoulli")
```


### Multiple regression in INLA
The logistic regression model using the required covariates was implemented in INLA. In previous part, we have standerdise the non-categorical covariates so that the mean is 1 and standered deviation is $1$ for each non-categorical covariate. For categorical covariates, INLA will automatically create $k-1$ indicators, where $k$ is the number of catogories for certain covariate. The link function is chosen to be the logit function in our model.

The results are shown below. The mean of the regression coefficients can be found in the first column of the summary results.
```{r}
#Priors for the regression coefficients.
prior.beta.logit <- list(mean.intercept = 0, prec.intercept =  0.31,
                    mean =0, 
                   prec = list(veh_value=5,exposure=6,
                               veh_body=0.3,veh_age=21,
                               gender=1,area=13.7,agecat=4.5))

#Fitting the model in INLA
#"control.fixed=prior.beta" sets regression coeff. priors.
results.car.logit <- inla(formula = clm ~ veh_value+exposure+veh_body+
               veh_age+gender+area+agecat, family="binomial",Ntrials=1,
             control.family=list(link="logit"),
             data=car.inla.data, control.fixed=prior.beta.logit,
             control.predictor = list(compute = TRUE,link=1),
             control.compute=list(config=TRUE,cpo=TRUE, dic=TRUE))
summary(results.car.logit)

```

As we created the reference group to be veh_body=BUS, gender=F, area=1,agecat=1, veh_age=1, the intercept $\beta_0$ reflects the scaled log odds ratio of this reference group. We can compute the mean of $p$ of the reference group, which is `r ilogit(-1.351)`. Hence, for the reference group, the probability of a claim is around $0.2$. For categorical covariate veh_body, the regression coefficient for each group is negative, indicating that all other vehicle categories have the probability of a claim lower than the BUS, and thus BUS is the one that most probable to have a claim. In addition, we see the regression coefficient for motorized caravan is the smallest among all the groups, which indicates that when other factors remain the same, motorized caravan is most likely to have a claim (apart from BUS). Similarly, we found the convertible vehicle is the least likely to have a claim, as the regression coefficient is the smallest among all the vehicle categories. For the covariate veh_age, we observe a increase trend in the regression coefficients. Therefore, we may believe that as the vehicle get older, the probability of a claim decreases. For the covariate gender, the regression coefficient is $-0.019$ for female. After taking the exponential, we can say the female have roughly $0.02$ less chance to claim than male. For covariate area, the regression coefficients for area B and F are positive, showing higher probabilities of a claim compared with the area A. The regression coefficients for area D and E are negative, showing lower probabilities of a claim compared with the area A. For the covariate agecat, we see the younger groups of people seem to have a larger probability to claim. 

For the non-categorical variable, we need to transfer the coefficients back to the original scale to better interpret the data. The transformed values are shown below. The odds ratios of the predictors can be calculated by exponentiating the estimated coefficients. For the vehicle value, we expect $4.57\%$ increase in the odds ratio of having a claim for every unit increase in the veh_value. Similarly, we expect $16.92\%$ increase in the odds ratio of having a claim for one unit increase in exposure, assuming all other predictors are fixed.

```{r}
cat("Regression coefficient for veh_value after transformation to the original scale: \n")
results.car.logit$summary.fixed$mean[2] * sd(dataCar$veh_value)
cat("Take the exponential to get the odds ratio: \n")
exp(results.car.logit$summary.fixed$mean[2] * sd(dataCar$veh_value))

cat("Regression coefficient for exposure after transformation to the original scale: \n")
results.car.logit$summary.fixed$mean[3] * sd(dataCar$exposure)
cat("Take the exponential to get the odds ratio: \n")
exp(results.car.logit$summary.fixed$mean[3] * sd(dataCar$exposure))
```

Now, we can compute the posterior mean of the model parameter ($p$) in Bernoulli distribution across different groups.

```{r}
fittedvaluesm.A <- results.car.logit$summary.fitted.values$mean
res.A <- car.data.scl %>% mutate(prob = ilogit(fittedvaluesm.A))
```

These are the posterior means of $p$ for different vehicle bodies. We see BUS has the highest claim probability while the convertible vehicle has the least probability to claim. Basically, we can get similar conclusion as the discussion in the previous part where the regression coefficients were discussed.
```{r}
res.A %>% 
  group_by(veh_body) %>%
  summarise(p.mu=mean(prob))
```

Female seems to have a slightly higher probability to claim. The difference is not significant though.
```{r}
res.A %>% 
  group_by(gender) %>%
  summarise(p.mu=mean(prob))
```
The oldest group of vehicle tends to be less likely to have a claim.
```{r}
res.A %>% 
  group_by(veh_age) %>%
  summarise(p.mu=mean(prob))
```

The younger groups of people tends to have a higher probability to claim.
```{r}
res.A %>% 
  group_by(agecat) %>%
  summarise(p.mu=mean(prob))
```

Area F is the area where the probaility of a claim is the largest across different area. Area D is where the probaility of a claim is the smallest.
```{r}
res.A %>% 
  group_by(area) %>%
  summarise(p.mu=mean(prob))
```


**b)[10 marks] Fit a Bayesian Poisson regression model on numclaims as
response with**

-   **log link function,**

-   **using veh_value, exposure, veh_body, veh_age, gender, area, and
    agecat as covariates.**

**Center and scale the non-categorical covariates.**

**Choose your own prior distributions (do not use default priors), and
explain the rationale your prior choices, and ensure that the posterior
is not too sensitive to your prior choice [Hint: look at the induced
prior on the linear predictor and the response.]**

**Compute the posterior means of the model parameters, and discuss the
results.**

We adopt a similar strategy for choosing priors as in part (a). However, here we look at the probability of each group instead of the odds ratio.
```{r}
mu.body <- c()
for (i in 1:13){
  body <- unique(car.inla.data$veh_body)[i]
  df.body <- car.inla.data %>% filter(veh_body %in% c(body)) 
  mu.body[i] <- mean(df.body$numclaims)
}
body.ratio <- max(mu.body)/min(mu.body)
body.ratio 

mu.gender <- c()
df.m <- car.inla.data %>% filter(gender %in% c("M"))
df.f <- car.inla.data %>% filter(gender %in% c("F"))
mu.gender[1] <- mean(df.m$numclaims)
mu.gender[2] <- mean(df.f$numclaims)
gender.ratio <- max(mu.gender)/min(mu.gender)
gender.ratio

mu.area <- c()
for (i in 1:6){
  a <- unique(car.inla.data$area)[i]
  df.area <- car.inla.data %>% filter(area %in% c(a)) 
  mu.area[i] <- mean(df.area$numclaims)
}
area.ratio <- max(mu.area)/min(mu.area)
area.ratio

mu.agecat <- c()
for (i in 1:6){
  age <- unique(car.inla.data$agecat)[i]
  df.agecat <- car.inla.data %>% filter(agecat %in% c(age)) 
  mu.agecat[i] <- mean(df.agecat$numclaims)
}
agecat.ratio <- max(mu.agecat)/min(mu.agecat)
agecat.ratio


mu.v_age <- c()
for (i in 1:4){
  v_age <- unique(car.inla.data$veh_age)[i]
  df.v_age <- car.inla.data %>% filter(veh_age %in% c(v_age)) 
  mu.v_age[i] <- mean(df.v_age$numclaims)
}
v_age.ratio <- max(mu.v_age)/min(mu.v_age)
v_age.ratio
```

For the choices of priors for the poisson regression, we first try to put $\mathcal{N}(0,1)$ on the intercept and $\mathcal{N}(0,0.1)$ on the other regression coefficients. The density of induced prior on model parameter $\mu$ in Poisson distribution is simulated below. We can see there is a hugh amount of probability mass between $0$ to $10$, as desired. We will continue with the choices of priors.
```{r}
beta0 <- rnorm(1000,sd=1)
beta1 <- rnorm(1000,sd=0.2)
beta2 <- rnorm(1000,sd=0.2)
beta3 <- rnorm(1000,sd=0.2)
beta4 <- rnorm(1000,sd=0.2)
beta5 <- rnorm(1000,sd=0.2)
beta6 <- rnorm(1000,sd=0.2)
beta7 <- rnorm(1000,sd=0.2)
x <- exp( beta0+0.4*beta1+beta2+beta3+beta4+beta5+beta6+beta7)
plot(density(x),main="density of the induced prior on the parameter mu in Poisson")
plot(density(exp(beta0)),main="density of the induced prior on the parameter mu in Poisson")
```

We fit the regression model in INLA using log as the link function. The results are summarised as follows.

```{r}

#Priors for the regression coefficients.
prior.beta.log <- list(mean.intercept = 0, prec.intercept =  1,
                    mean =0, 
                   prec = 25)
#Fitting the model in INLA
#"control.fixed=prior.beta" sets regression coeff. priors.
results.car.log <- inla(formula = numclaims ~ veh_value+exposure+veh_body+
               veh_age+gender+area+agecat, family="poisson",
             control.family=list(link="log"),
             data=car.inla.data, control.fixed=prior.beta.log,
             control.predictor = list(compute = TRUE,link=1),
             control.compute=list(config=TRUE,cpo=TRUE, dic=TRUE))
summary(results.car.log)
```
For interpreting the coefficients we can exponentiate them. for the intercept, $e^{-2.451}=0.086$ represents the average number of claims of the reference category (i.e. the group with veh_body=BUS , veh_age=1, gender=M, area=A and agecat=1). The exponential function applied to the coefficients of the categorical factors provides the relative rate of the number of claims for different categories. For example, looking at the group veh_body=CONVT, $e^{-0.131}=0.877$, thus there is a $12.3\%$ decrease in the average rate of claims for convertible vehicle compared to the buses. The same interpretation can be extended to the other categorical covariates.


For the non-categorical variable, we need to transfer the coefficients back to the original scale to better interpret the data. The transformed values are shown below. For the vehicle value, we expect $3.59\%$ increase of the expected number of claims for every unit increase in the veh_value. Similarly, we expect $16.22\%$ increase of the expected number of claims for every unit increase in the exposure. 

```{r}
cat("Regression coefficient for veh_value after transformation to the original scale: \n")
results.car.log$summary.fixed$mean[2] * sd(dataCar$veh_value)
cat("Take the exponential:\n")
exp(results.car.log$summary.fixed$mean[2] * sd(dataCar$veh_value))

cat("Regression coefficient for exposure after transformation to the original scale: \n")
results.car.log$summary.fixed$mean[3] * sd(dataCar$exposure)
cat("Take the exponential:\n")
exp(results.car.log$summary.fixed$mean[3] * sd(dataCar$exposure))
```

Now, we can compute the posterior mean of the model parameter ($\mu$) in Poisson distribution across different groups.

```{r}
fittedvaluesm.B <- results.car.log$summary.fitted.values$mean
res.B <- car.data.scl %>% mutate(numclaims.mu = exp(fittedvaluesm.B))
```

We summarise the posterior average number of claims across different groups. For different vehicle bodies, panel van seems to have the largest number of claims $1.096$, while the least number of claims is given by utility. 
```{r}
res.B %>% 
  group_by(veh_body) %>%
  summarise(mean_num_claims=mean(numclaims.mu))
```

There is no huge differences of female and male in terms of the mean number of claims made. 
```{r}
res.B %>% 
  group_by(gender) %>%
  summarise(mean_num_claims=mean(numclaims.mu))
```
For the covariate are, the area F has the largest mean number of claims while the area D has the smallest number of claims.
```{r}
res.B %>% 
  group_by(area) %>%
  summarise(mean_num_claims=mean(numclaims.mu))
```

The younger people tend to make slightly more claims on average, which is consistent with the conclusion in part a that they also have a higher probability of having a claim.
```{r}
res.B %>% 
  group_by(agecat) %>%
  summarise(mean_num_claims=mean(numclaims.mu))
```

The veh_age=2 group gets the highest number of claims on average while the veh_age=4 group gets the lowest number of claims on average. Groups veh_age=1 and veh_age=3 have similar values of mean number of claims.
```{r}
res.B %>% 
  group_by(veh_age) %>%
  summarise(mean_num_claims=mean(numclaims.mu))
```



**c)[10 marks]** **Fit a zero-inflated Bayesian Poisson regression model
(<https://en.wikipedia.org/wiki/Zero-inflated_model>) on**

-   **numclaims as response,**

-   **with log link function,**

-   **using veh_value, exposure, veh_body, veh_age, gender, area, and
    agecat as covariates.**

**Center and scale the non-categorical covariates.**

**Choose your own prior distributions (do not use default priors), and
explain the rationale your prior choices, and ensure that the posterior
is not too sensitive to your prior choice [Hint: look at the induced
prior on the linear predictor and the response.]**

**Compute the posterior means of the model parameters, and discuss the
results.**

First, we look at the number of claims in the dataset. The observed proportion of zero claims is $0.932$, indicating there is a large number of zeros in the response variable (numclaims). Thus, we fit zero-inflated Poisson regression model in INLA, by setting the argument `family = "zeroinflatedpoisson1"`.
```{r}
round(prop.table(table(car.inla.data$numclaims)),5)
```


```{r}
#Priors for the regression coefficients.
prior.beta.poi <- list(mean.intercept = 0, prec.intercept =  1,
                       mean =0, prec = 25)
#Fitting the model in INLA
#"control.fixed=prior.beta" sets regression coeff. priors.
results.car.poi <- inla(formula = numclaims ~ veh_value+exposure+veh_body+
               veh_age+gender+area+agecat, family="zeroinflatedpoisson1",
             control.family=list(link="log"),
             data=car.inla.data, control.fixed=prior.beta.poi,
             control.predictor = list(compute = TRUE),
             control.compute=list(config=TRUE,cpo=TRUE, dic=TRUE))
summary(results.car.poi)
```

From the results, we can see the probability of having no claims is $0.297$, with a $95\%$ credible interval $(0.219,0.299)$. The probability of having claims is thus $0.703$, and the number of claims are given by the model process specified in the question. Notice that the intercept is $-2.101$. After taking the exponential, the we get $e^{-2.101}=0.1223$ for zero-inflated Poisson regression. We can thus compute the expectation of the number of claims of the reference group by $0.703\times 0.1223+0.297 \times 0=0.0860$. Note that this is slightly lower than the mean number of claims of the reference group given by the model in part (b) (which is $0.0862$). The interpretations of the regression coefficients are similar as part (b), except we are now conditioning on that there is a claim (or claims). Otherwise, the probability of no claim is $0.297$.

We can also compute the posterior mean number of claims of different categories. All the values are now generally lower than values we got in part (b) using standard Poisson GLM. Moreover, we observe similar facts as in part (b); e.g. for different vehicle bodies, panel van seems to have the largest mean number of claims, while the least mean number of claims is given by utility. 

```{r}
fittedvaluesm.C <- results.car.poi$summary.fitted.values$mean
res.C <- car.data.scl %>% mutate(numclaims.mu = exp(fittedvaluesm.C))
```

```{r}
res.C %>% 
  group_by(veh_body) %>%
  summarise(mean_num_claims=mean(numclaims.mu)*0.703)
```

```{r}
res.C %>% 
  group_by(gender) %>%
  summarise(mean_num_claims=mean(numclaims.mu))
```

```{r}
res.C %>% 
  group_by(area) %>%
  summarise(mean_num_claims=mean(numclaims.mu))
```

```{r}
res.C %>% 
  group_by(agecat) %>%
  summarise(mean_num_claims=mean(numclaims.mu))
```

```{r}
res.C %>% 
  group_by(veh_age) %>%
  summarise(mean_num_claims=mean(numclaims.mu))
```





**d)[10 marks] Fit a new model on numclaims in terms of the same
covariates to improve on the models in part b) or part c) by considering
interactions between covariates, as well as random effects. Describe
your new model and justify your choices.**

**Choose your own prior distributions (do not use default priors), and
explain the rationale your prior choices, and ensure that the posterior
is not too sensitive to your prior choice [Hint: look at the induced
prior on the linear predictor and the response.]**

**Compute the posterior means of the model parameters, and discuss the
results.**

We have fit a new model on numclaims in terms of the same covariates to improve on the models in part c) by considering interactions between the veh_value and exposure, as well as random effects across areas. This is because we think there might be interactions between the vehicle value and the exposure, and there might be different random effects across different area. The priors and link function used here are the same as those in part c). For the rationale, see previous explanations.

The results are shown below, where we can see the DIC decrease slightly compared with the previous model in part(c). The posterior means of model parameters can be found in the first column, and we can see them quite similar to those given by part(c). 

```{r}
#Priors for the regression coefficients.
prior.beta.poi <- list(mean.intercept = 0, prec.intercept =  1,
                       mean =0, prec = 25)

#Fitting the model in INLA
results.car.poi.rd <- inla(formula = numclaims ~ veh_value*exposure+
                             veh_body+veh_age+gender+agecat+
                             f(area, model="iid"), 
               family="zeroinflatedpoisson1",
             control.family=list(link="log"),
             data=car.inla.data, control.fixed=prior.beta.poi,
             control.predictor = list(compute = TRUE,link=1),
             control.compute=list(config=TRUE, dic=TRUE, cpo=TRUE))
summary(results.car.poi.rd)
```




**e)[10 marks] Perform posterior predictive model checks for your models
b, c, d (i.e. using replicates).**

**As test functions, use the number of rows in the dataset with
numclaims equal 0, 1, 2, 3, and 4 (5 test functions).**

**Compute the RMSE values for predicting numclaims based on all 3
models.**

**Discuss the results.**

We sampled the posterior values of the linear predictor of the three models, and then used the exponentiated values to generate posterior predictive replicates.  the posterior replicates in the Stan model code in the generated quantities block. We try
3 test functions: the median, skewness, and the mean square difference, which we define as the mean of
((y_obs[i]-y_obs[i+1])ˆ2) from i = 1 to n_obs-1 (n_obs is the number of days where we observed the
exchange rate). This mean square function is related to the volatility of the exchange rate, so it is a relevant
quantity to test our model on.

```{r}
nbsamp <- 1000
n <- nrow(car.inla.data)
yrep.b <- matrix(0,nrow=n,ncol=nbsamp)
yrep.c <- matrix(0,nrow=n,ncol=nbsamp)
yrep.d <- matrix(0,nrow=n,ncol=nbsamp)

car.b.samples=inla.posterior.sample(n=nbsamp, result=results.car.log)
car.c.samples=inla.posterior.sample(n=nbsamp, result=results.car.poi)
car.d.samples=inla.posterior.sample(n=nbsamp, result=results.car.poi.rd)

predictor.b.samples=inla.posterior.sample.eval(function(...) {Predictor},
car.b.samples)
predictor.c.samples=inla.posterior.sample.eval(function(...) {Predictor},
car.c.samples)
predictor.c.samples=inla.posterior.sample.eval(function(...) {},
car.c.samples)
predictor.d.samples=inla.posterior.sample.eval(function(...) {Predictor},
car.d.samples)

for (row.num in 1:n){   
  yrep.b[row.num,]<- rpois(n=nbsamp,
                         lambda=exp(predictor.b.samples[row.num,]))
  
  yrep.c[row.num,]<- rzipois(n=nbsamp,
                             lambda=exp(predictor.c.samples[row.num,]),
                             pi=car.c.samples[[1000]]$hyperpar)  

  yrep.d[row.num,]<- rpois(n=nbsamp,
                         lambda=exp(predictor.d.samples[row.num,]))  

}
```

```{r}
rmse(car.inla.data$numclaims,rowMeans(yrep.b))
rmse(car.inla.data$numclaims,rowMeans(yrep.c))
rmse(car.inla.data$numclaims,rowMeans(yrep.d))
```
```{r}
car.post.pred.test <- function(replicates,car.inla.data){
  no.clm0 <- colSums(replicates == 0)
  true.no.clm0 <- sum(car.inla.data$numclaims==0)
  
  no.clm1 <- colSums(replicates == 1)
  true.no.clm1 <- sum(car.inla.data$numclaims==1)
  
  no.clm2 <- colSums(replicates == 2)
  true.no.clm2 <- sum(car.inla.data$numclaims==2)
  
  no.clm3 <- colSums(replicates == 3)
  true.no.clm3 <- sum(car.inla.data$numclaims==3)
  
  no.clm4 <- colSums(replicates == 4)
  true.no.clm4 <- sum(car.inla.data$numclaims==4)

  par(mfrow=c(2,3))
  par(mar=c(2,2,2,2))
  
  xmin=min(c(no.clm0,true.no.clm0))-2
  xmax=max(c(no.clm0,true.no.clm0))+2
  hist(no.clm0,col="gray40",main="number of claim=0",xlim=c(xmin,xmax))
  abline(v=true.no.clm0,col="red",lwd=2)
  
  xmin=min(c(no.clm1,true.no.clm1))-2
  xmax=max(c(no.clm1,true.no.clm1))+2
  hist(no.clm1,col="gray40",main="number of claim=1",xlim=c(xmin,xmax))
  abline(v=true.no.clm1,col="red",lwd=2)
  
  xmin=min(c(true.no.clm2,no.clm2))-20
  xmax=max(c(no.clm2,true.no.clm2))+20
  hist(no.clm2,col="gray40",main="number of claim=2")
  abline(v=true.no.clm2, col="red", lwd=2)
  
  xmin=min(c(no.clm3,true.no.clm3))-2
  xmax=max(c(no.clm3,true.no.clm3))+2
  hist(no.clm3,col="gray40",main="number of claim=3",xlim=c(xmin,xmax))
  abline(v=true.no.clm3,col="red",lwd=2)
  
  xmin=min(c(no.clm4,true.no.clm4))-2
  xmax=max(c(no.clm4,true.no.clm4))+2
  hist(no.clm4,col="gray40",main="number of claim=4",xlim=c(xmin,xmax))
  abline(v=true.no.clm4,col="red",lwd=2)
}


```


```{r}
car.post.pred.test(yrep.b,car.inla.data)
car.post.pred.test(yrep.c,car.inla.data)
```



Explanation (min 300 characters in your own words, otherwise -5 marks
for insufficient explanation):

![](barcelona.jpg)

**Problem 2 - Barcelona study**

**In this problem, we will use a dataset from the CitieS-Health project
that provides insight into the impact of air pollution on humans. It is
comprised of data collected in Barcelona, Spain, and examines various
environmental variables, such as air pollution levels, and their effects
on mental health and wellbeing. In addition to environmental factors,
this dataset also captures self-reported survey data on mental health,
physical activity, diet habits, and more. From performance in a Stroop
test (a type of psychological test evaluating attention capacity and
processing speed) to information on total noise exposure at 55 dB - this
dataset contains interesting information to understand the link between
air pollution and human health.**

**We start by loading the dataset.**

```{r}
 study<-read.csv("Barcelona.csv")
 head(study)
```

**Descriptions of some of the covariates:**

| **Column name**                     | **Description**                                                                                    |
|-------------------------|-----------------------------------------------|
| **Person_ID**                       | ID of person filling out the survey (integer). Multiple rows for most persons, at different dates. |
| **date_all**                        | Date of the survey. (Date)                                                                         |
| **year**                            | Year of the survey. (Integer)                                                                      |
| **month**                           | Month of the survey. (Integer)                                                                     |
| **day**                             | Day of the survey. (Integer)                                                                       |
| **dayoftheweek**                    | Day of the week of the survey. (Integer)                                                           |
| **hour**                            | Hour of the survey. (Integer)                                                                      |
| **sadness**                         | Sadness score. (Integer)                                                                           |
| **wellbeing**                       | Self-reported survey responses regarding wellbeing. (Integer)                                      |
| **energy**                          | Self-reported survey responses regarding energy levels. (Integer)                                  |
| **stress**                          | Self-reported survey responses regarding stress levels. (Integer)                                  |
| **sleep**                           | Self-reported survey responses regarding sleep quality. (Integer)                                  |
| **hours_out**                       | Self-reported survey responses regarding time spent outdoors. (Integer)                            |
| **computer_use**                    | Self-reported survey responses regarding computer use. (Yes/No)                                    |
| **on_a\_diet**                      | Self-reported survey responses regarding diet. (Yes/No)                                            |
| **alcohol**                         | Self-reported survey responses regarding alcohol consumption. (Yes/No)                             |
| **drugs**                           | Self-reported survey responses regarding drug use. (Yes/No)                                        |
| **sick**                            | Self-reported survey responses regarding illness. (Yes/No)                                         |
| **other_factors**                   | Self-reported survey responses regarding other factors. (Yes/No)                                   |
| **stroop_test_performance**         | Performance in the Stroop test. (Float)                                                            |
| **no2bcn_24h**                      | Nitrogen dioxide (NO2) levels in Barcelona over 24 hours. (Float)                                  |
| **no2bcn_12h**                      | Nitrogen dioxide (NO2) levels in Barcelona over 12 hours. (Float)                                  |
| **no2gps_24h**                      | Nitrogen dioxide (NO2) levels in GPS locations over 24 hours. (Float)                              |
| **no2gps_12h**                      | Nitrogen dioxide (NO2) levels in GPS locations over 12 hours. (Float)                              |
| **no2bcn_12h_x30**                  | Nitrogen dioxide (NO2) levels in Barcelona over 12 hours multiplied by 30. (Float)                 |
| **no2bcn_24h_x30**                  | Nitrogen dioxide (NO2) levels in Barcelona over 24 hours multiplied by 30. (Float)                 |
| **no2gps_12h_x30**                  | Nitrogen dioxide (NO2) levels in GPS locations over 12 hours multiplied by 30. (Float)             |
| **no2gps_24h_x30**                  | Nitrogen dioxide (NO2) levels in GPS locations over 24 hours multiplied by 30. (Float)             |
| **min_gps**                         | Minimum GPS location. (Float)                                                                      |
| **district**                        | District of Barcelona where the survey was conducted. (String)                                     |
| **education**                       | Educational level of the participant. (String)                                                     |
| **maxwindspeed_12h**                | Maximum wind speed over 12 hours. (Float)                                                          |
| **access_greenbluespaces_300mbuff** | Access to green and blue spaces within a 300m buffer. (Yes/No)                                     |
| **microgram3**                      | Micrograms per cubic meter of pollutants. (Float)                                                  |
| **age_yrs**                         | Age of the participant in years. (Integer)                                                         |
| **yearbirth**                       | Year of birth of the participant. (Integer)                                                        |
| **smoke**                           | Self-reported survey responses regarding smoking status. (Yes/No)                                  |
| **gender**                          | Gender of the participant. (Woman/Man)                                                             |
| **hour_gps**                        | Hour of the GPS location. (Integer)                                                                |
| **pm25bcn**                         | Particulate matter (PM2.5) levels in Barcelona. (Float)                                            |
| **BCmicrog**                        | Black carbon (BC) levels in micrograms. (Float)                                                    |
| **sec_noise55_day**                 | Seconds of noise over 55 minutes in a day. (Integer)                                               |
| **sec_noise65_day**                 | Seconds of noise over 65 minutes in a day. (Integer)                                               |
| **tmean_24h**                       | Mean temperature over 24 hours. (Float)                                                            |
| **tmean_12h**                       | Mean temperature over 12 hours. (Float)                                                            |
| **humi_24h**                        | Humidity over 24 hours. (Float)                                                                    |
| **humi_12h**                        | Humidity over 12 hours. (Float)                                                                    |
| **pressure_24h**                    | Pressure over 24 hours. (Float)                                                                    |
| **pressure_12h**                    | Pressure over 12 hours. (Float)                                                                    |
| **precip_24h**                      | Precipitation over 24 hours. (Float)                                                               |
| **precip_12h**                      | Precipitation over 12 hours. (Float)                                                               |
| **precip_12h_binary**               | Binary value for precipitation over 12 hours. (Integer)                                            |
| **precip_24h_binary**               | Binary value for precipitation over 24 hours. (Integer)                                            |
| **maxwindspeed_24h**                | Maximum wind speed over 24 hours. (Float)                                                          |

**You can use either JAGS, Stan, or INLA for this question.**

**a)[10 marks] Fit a Bayesian linear regression model**

-   **on the logarithm of stroop_test_performance as response,**

-   **using the following covariates: gender, on_a\_diet, alcohol,
    drugs, sick, other_factors, educational, smoke, no2gps_24h,
    maxwindspeed_24h, precip_24h, sec_noise55_day,
    access_greenbluespaces_300mbuff, age_yrs, tmean_24h (you can use
    categorical covariates by converting integers to factors if
    appropriate).**

**Center and scale the non-categorical covariates.**

**Choose your own prior distributions (do not use default priors), and
explain the rationale your prior choices, and ensure that the posterior
is not too sensitive to your prior choice [Hint: look at the induced
prior on the response.]**

**Compute the posterior means of the model parameters, and interpret
their meaning.**

Data preprocessing.
```{r}
study.data.scl <- study %>% 
  select(gender, on_a_diet, alcohol, drugs, sick, other_factors, 
         education, smoke,no2gps_24h, maxwindspeed_24h,precip_24h,
         sec_noise55_day,access_greenbluespaces_300mbuff,age_yrs,
         tmean_24h, stroop_test_performance, sadness) %>%
  mutate(no2gps_24h=scale(no2gps_24h),maxwindspeed_24h=scale(maxwindspeed_24h),
         precip_24h=scale(precip_24h),sec_noise55_day=scale(sec_noise55_day),
         age_yrs=scale(age_yrs),tmean_24h=scale(tmean_24h),
         log.stroop_test_performance=scale(log(stroop_test_performance)))

```

```{r}
x <- matrix(nrow=1000,ncol=15)
x[,1] <- rnorm(1000,sd=sqrt(10))
for (i in 2:15){
  x[,i] <- rnorm(1000,sd=sqrt(1))
}
plot(density((rowSums(x))))

```



```{r}
#Priors for the regression coefficients.
prior.beta.a <- list(mean.intercept = 0, prec.intercept =  0.1,
                       mean =0, prec = 1)

prec.prior <- list(prec=list(prior = "loggamma", param = c(0.1, 0.1)))
#Fitting the model in INLA
#"control.fixed=prior.beta" sets regression coeff. priors.
results.study.a <- inla(formula = log.stroop_test_performance ~ gender+on_a_diet+
                          alcohol+drugs+sick+other_factors+education+smoke+
                          no2gps_24h+maxwindspeed_24h+precip_24h+sec_noise55_day+
                          access_greenbluespaces_300mbuff+age_yrs+tmean_24h,
                        family="gaussian",
                        control.family=list(hyper=prec.prior),
                        data=study.data.scl, control.fixed=prior.beta.a,
                        control.predictor = list(compute = TRUE),
                        control.compute=list(config=TRUE,cpo=TRUE, dic=TRUE))
summary(results.study.a)
```




Explanation (min 300 characters in your own words, otherwise -5 marks
for insufficient explanation):

**b)[10 marks] Fit a Bayesian Poisson GLM**

-   **for sadness as response,**

-   **log link function,**

-   **using the following covariates: gender, on_a\_diet, alcohol,
    drugs, sick, other_factors, educational, smoke, no2gps_24h,
    maxwindspeed_24h, precip_24h, sec_noise55_day,
    access_greenbluespaces_300mbuff, age_yrs, tmean_24h (you can use
    categorical covariates by converting integers to factors if
    appropriate).**

**Center and scale the non-categorical covariates.**

**Choose your own prior distributions (do not use default priors), and
explain the rationale your prior choices, and ensure that the posterior
is not too sensitive to your prior choice [Hint: look at the induced
prior on the response.]**

**Compute the posterior means of the model parameters, and interpret
their meaning.**

```{r}
mm=model.matrix(sadness~  gender+on_a_diet+
                          alcohol+drugs+sick+other_factors+education+smoke+
                          no2gps_24h+maxwindspeed_24h+precip_24h+sec_noise55_day+
                          access_greenbluespaces_300mbuff+age_yrs+tmean_24h,data=study.data.scl)
var.beta=5/quantile(rowSums(mm^2),0.5)
cat("Variance of regression coefficients:",var.beta, "\n")

```

```{r}
x <- matrix(nrow=1000,ncol=15)
x[,1] <- rnorm(1000,mean=0,sd=sqrt(2))
for (i in 2:15){
  x[,i] <- rnorm(1000,mean=0,sd=sqrt(0.5 ))
}
plot(density((rowSums(x))),xlim=c(-5,50))
plot(density((x[,1])),xlim=c(-5,50))
```
```{r}
glm(sadness ~ gender+on_a_diet+
                          alcohol+drugs+sick+other_factors+education+smoke+
                          no2gps_24h+maxwindspeed_24h+precip_24h+sec_noise55_day+
                          access_greenbluespaces_300mbuff+age_yrs+tmean_24h,
    family=poisson(link="log"),data=study.data.scl)
```


```{r}
#Priors for the regression coefficients.
prior.beta.b <- list(mean.intercept = 0, prec.intercept =  1/10,
                       mean =0, prec = 1)


#Fitting the model in INLA
#"control.fixed=prior.beta" sets regression coeff. priors.
results.study.b <- inla(formula = sadness ~ gender+on_a_diet+
                          alcohol+drugs+sick+other_factors+education+smoke+
                          no2gps_24h+maxwindspeed_24h+precip_24h+sec_noise55_day+
                          access_greenbluespaces_300mbuff+age_yrs+tmean_24h,
                        family="poisson",
                        control.family=list(link="log"),
                        data=study.data.scl, control.fixed=prior.beta.b,
                        control.predictor = list(compute = TRUE,link=1),
                        control.compute=list(config=TRUE,cpo=TRUE, dic=TRUE))
summary(results.study.b)
```

Explanation (min 300 characters in your own words, otherwise -5 marks
for insufficient explanation):

**c)[10 marks] Incorporate Person_ID as a random effects into the models
a.) and b.).**

**Choose your own prior distributions for this random effect (do not use
default priors).**

**Compare the posterior means of the parameter values with a) and b).**

**Discuss the changes that happened due to using random effects.**

```{r}

```

Explanation (min 300 characters in your own words, otherwise -5 marks
for insufficient explanation):

**d)[10 marks] Do posterior predictive checks (i.e. using replicates)
for the sadness score for your models with or without random effects.
Explain the choice of test functions that you used.**

**Compute the posterior means of the response variable using the
original covariates, and use this to compute the RMSE values for both
models (i.e. with, or without random effects).**

**Discuss the results.**

```{r}

```

Explanation (min 300 characters in your own words, otherwise -5 marks
for insufficient explanation):

**e)[10 marks]**

**Plot the posterior predictive distributions for
stroop_test_performance and sadness for the random effect models in part
c) for the following new person in the dataset:**

**Person_ID=286, gender="Woman", on_a\_diet="Yes", alcohol="No",
drugs="No", sick="No", other_factors="No", education="University",
smoke="Yes", no2gps_24h=80, maxwindspeed_24h=10, precip_24h=50,
sec_noise55_day=10000, access_greenbluespaces_300mbuff="Yes",
age_yrs=40, tmean_24h=25**

**In the case of stroop_test_performance, plot the estimated density,
while for sadness, plot a histogram.**

**Compute the posterior predictive mean, and standard deviation.**

**Discuss the results.**

```{r}

```

Explanation (min 300 characters in your own words, otherwise -5 marks
for insufficient explanation):
